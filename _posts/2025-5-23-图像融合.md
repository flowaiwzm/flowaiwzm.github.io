#### 图像融合基本知识

---

如今现有的图像融合方法主要集中在视觉增强的架构设计和训练策略上，很少考虑对下游视觉感知任务的适应。

**主要面临的挑战在于图像融合时的像素错位和对抗性攻击**

- **像素错位**--像素错位是一种图像处理或计算机视觉中的现象，指图像中像素的位置发生*非预期的偏移或扭曲，导致视觉上的失真或结构异常*
  - **出现的原因：**
    - 几何变换
    - 数据传输错误
    - 硬件问题
- **对抗性攻击**--对机器学习模型（尤其是深度学习）的恶意输入干扰，通过添加人眼难以察觉的**噪声**（对抗样本），使模型输出错误结果；跨模态攻击
  - **攻击类型**
    - 白盒攻击
    - 黑盒攻击
    - 目标攻击

---

融合过程中并没有将图像中**全面的语义信息**得到充分探索；

- **纯融合方法（Fusion for Visual Enhancement)**
  - Auto-Encoder(VUE自动编码器)
  - GAN(对抗生成网络)
  - CNN（卷积神经网络）
  - Transformer（变压器）
- **数据兼容方法（Data Compatible）**
  - Registration
  - Attack
  - General
- **面向应用方法（Application-oriented）**
  - Perception



---

**Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and  Interactive Image Fusion**

利用**语义文本引导图像融合模型进行降级感知和交互式图像融合任务**，称为 Text-IF。它创新性地将经典图像融合扩展到文本引导图像融合，并能够和谐地解决融合过程中的退化和交互问题。通过文本语义编码器和语义交互融合解码器，Text-IF 可用于一体化红外和可见光图像降级感知处理和交互式灵活融合结果。通过这种方式，Text-IF 不仅实现了多模态图像融合，而且实现了多模态信息融合



多模态图像能更加全面有效的表示图像；**可见光图片**作为重要的代表，提供基于放射率的视觉信息；类似于人类的视觉；儿红外图像提供基于热辐射的信息，对于检测热目标和观察夜间活动更有价值。红外和可见光图像融合侧重于融合红外和可见光图像的互补信息，产生高质量的融合图像 

受环境条件的限制，最初采集的红外和可见光图像可能会出现**退化，融合图像质量低**。可见光图像容易受到劣化问题的影响，例如低光、过度曝光等。红外图像不可避免地会受到噪声（包括热噪声、电子噪声和环境噪声）、对比度降低和其他相关影响的影响。目前的融合方法缺乏自适应求解退化的能力，导致融合图像质量低下。此外，依靠人工预处理来增强图像存在灵活性和效率问题 [29]。因此，研究一种协调退化感知处理和交互式融合的模型具有实际意义。



Text-IF 包含图像管道和文本交互引导架构，包括文本语义编码器和语义交互引导模块。在图像融合管道中，我们精心设计了基于 Transformer 的图像提取模块和交叉融合层，以实现高质量的融合。在文本语义编码器中，我们聚合了强大的预训练视觉语言模型的文本语义提取能力。通过语义交互引导模块，将文本的语义特征与图像融合特征耦合在一起，实现文本引导图像融合的目标。解决了现有图像融合方法难以适应有退化的复杂场景融合，只能输出相对固定的结果而没有交互性的问题



- 为了适应复杂的退化条件，我们解决了图像融合和退化感知处理的综合问题。它突破了图像融合中提质的局限。
- 引入了一个语义交互指导模块，将文本和图像的信息融合在一起。所提出的方法不仅实现了多模态图像融合，而且实现了多模态信息融合
- 增加了定制融合结果的自由度。它提供交互式融合，无需事先专业知识或预定义规则即可生成更灵活、高质量和用户所需的结果。



**相关工作：**

1、CSR采用卷积稀疏表示进行图像融合，提取多层特征，利用这些特征来生成融合图像；基于CNN卷积神经网络的端到端融合结构，使融合过程更加灵活和直接；

2、U2Fusion采用密集连接网络生成以源图像为条件的融合图像，实现多合一的图像融合方法

3、DDFM扩散模型进行融合

4、文本图像模型，随着 Transformer 和表示学习的进步，再加上大型数据集的支持，多模态文本引导图像模型取得了成功。CLIP [25] 建立在两个基于神经网络的编码器之上，这两个编码器使用对比损失来对齐图像和文本对。由于广泛的数据和无监督训练，它具有强大的零样本识别和强大的文本、图像特征提取能力。在 CLIP 模型的支持下，已经提出了许多文本驱动图像生成和处理的方法。Style-CLIP [23] 为 StyleGAN [9] 设计了一个文本引导界面，允许使用文本提示更改真实图像。除了 GAN 模型之外，具有文本条件的扩散模型也引起了很多关注。DiffusionCLIP [11] 提出了带有 CLIP 的扩散模型，用于文本驱动的图像处理。此外，稳定扩散 [26] 将扩散模型与Text Encoder 和 Attention 机制实现文本控制图像生成的效果。通过文本引导，可以自定义图像生成、图像处理等任务的效果，实现交互式多模态融合控制。现有的图像融合方法在复杂场景面临退化时束手无策。即使配备 SOTA 图像修复模型，它也很麻烦且做得不好。此外，没有专业知识的用户很难实现交互式的高质量图像融合。因此，有必要创新性地引入文本引导图像融合框架，以便简单使用。

![image-20250523200017806](C:\Users\lwj\AppData\Roaming\Typora\typora-user-images\image-20250523200017806.png)

**模型组成**

- **Image Encoder 图像编码器**
  - 采用两个Transformer模块提取两个源图像基本特征
- **Cross Fusion Layer 交叉融合层**
  - 整合不同模态的特征信息来获取不同模态的特征
- **Semantic Interaction Fusion Decoder 语义交互融合解码器**
  - 它由基于 Transformer 的解码器块和语义交互指导模块 （SIGM） 构建
- **Text Interaction Guidance Architecture 文本交互指导体系**
  - **Text Semantic Encoder文本语义编码器**---该文本 Text 提供了相应的语义特征，以指导图像融合网络获得指定的融合结果（例如，指定任务类型和降级类型)
  - **Semantic Interaction Guidance Module (SIGM) 语义交互指导模块**

**模型实现细节**

- 采用学习率0.001的Adam优化器
- batch-size为16
- 源图像裁剪成96×96尺寸
- 可学习参数$αint(t)$, $αSSIM (t)$, $αgrad(t)$, $αcolor(t)$
- 采用的开源数据集MSRS，MFNet, RoadScene, LLVIP 采用3618对作训练集，1135作为测试集

**实验模型与UMF-CMGR,  TarDAL , ReCoNet, MURF , U2Fusion ,  MetaFusion， DDFM对比结果**

---

**RGB转YCbCr色彩空间**

---

**损失值真的只是采用累加的方式来表示/还是需要采用权重着重表示**

---

**森林火灾烟雾太大无法确认着火点，而红外图像热成像图像无法提供有效的参考位置，只知道大致位置**

---

**编码器A与编码器B的特征融合是否可以采用更加高效选择的融合手段而不是简单连接后投影融合两种特征**

---







---

**MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding  from Object Detection（实现融合网络与目标检测网络之间通过引入元特征嵌入网络实现优化）**

---

**SwinFusion: Cross-domain Long-range Learning for General Image Fusion via Swin Transformer进行通用图像融合的跨域远程学习**



