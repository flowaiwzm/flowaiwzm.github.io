#### 常见的编码方式

| 编码方式                             | 长程依赖 | 多尺度 | 计算效率 | 表达能力 | 常见用途       |
| ------------------------------------ | -------- | ------ | -------- | -------- | -------------- |
| CNN 编码器（ResNet等）               | 弱       | 强     | 高效     | 中等     | 分类、检测     |
| Transformer 编码器（ViT、Restormer） | 强       | 中等   | 昂贵     | 强       | 图像恢复、融合 |
| 多分支编码器（Fusion）               | 强       | 可定制 | 适中     | 高       | 多模态融合     |
| 金字塔编码器（FPN、UNet）            | 中       | 强     | 高效     | 高       | 分割、超分辨率 |

- **CNN编码器**
  - **VGG，ResNet,DenseNet、EfficientNet、UNet网络**
    - **特点：**逐层卷积 + 下采样（如最大池化、stride卷积）
    - **优势：**计算高效，局部建模强，易于训练
    - **局限：**感受野有限，长程建模弱
- **Transformer 编码器（当前主流）**
  - **ViT、Swin Transformer、Restormer、TransUnet网络**
    - **特点**：Patch Embed + Self-Attention + FFN
    - **优势：**长距离依赖建模强，支持全局感受野
    - **局限：**训练难度大，参数量较大
    - **Restormer：更适合图像恢复（空间自注意力 + CNN 残差）**
    - **Swin Transformer：层级化窗口注意力，自然适配图像分辨率变化**
- **多分支编码器（常见于图像融合任务）**
  - **Dual-stream Encoder**
    - **特点：**输入 A（红外）→ Encoder_A，输入 B（可见光）→ Encoder_B
    - **优势：**各自提取专属特征，后期融合效果更灵活
    - **多用于：**红外-可见光、近红外-全色图像、结构-纹理融合等任务
- **金字塔式编码器（多尺度特征）**
  - **FPN（特征金字塔网络）、HRNet、UNet 编码器部分**
    - **特点：**多个下采样分支，构成金字塔形多尺度表示



结构：OverlapPatchEmbed → 多层 TransformerBlock（分 4 个 stage）→ 每个 stage 之间 Downsample
特点：
- 强局部建模（通过 Overlap Patch）
- 多尺度表示
- Transformer 提供远距离建模能力
适用于：图像融合、超分辨率、去噪、图像恢复等任务

---

**OverlapPatchEmbed**相比于传统的**PatchEmbed**的优势在于：

| 作用                               | 解释                                       |
| ---------------------------------- | ------------------------------------------ |
| ✅ 保留边缘信息                     | 重叠区域避免了 patch 边界信息丢失          |
| ✅ 增强局部建模能力                 | 类似 CNN 的滑动感受野，有助于学习纹理/细节 |
| ✅ 提高稳定性                       | 对于高分辨率图像和图像重建等任务尤为重要   |
| ✅ 更好融合 CNN 与 Transformer 特性 | 是 Restormer、SwinIR 等结构常用组件        |



---

**主要优势：Swin Transformer 相比标准 Transformer 的优点**

| 比较维度             | 普通 ViT-style Transformer   | Swin Transformer                      |
| -------------------- | ---------------------------- | ------------------------------------- |
| **计算效率**         | 全局自注意力（复杂度 O(N²)） | 局部滑窗自注意力（复杂度 O(N)）       |
| **建模能力**         | 全局建模强，但效率低         | 局部窗口建模 + 移动窗口捕捉跨区域依赖 |
| **适应大图像**       | 不易扩展到高分辨率图像       | 适用于大图像（例如 256×256、512×512） |
| **层级结构**         | 无原生金字塔结构             | 多层级金字塔结构（更接近 CNN）        |
| **位置编码**         | 通常需要位置编码（难泛化）   | 相对位置偏置，泛化能力强              |
| **视觉特征提取能力** | 弱于 CNN 和 Swin 在细节表达  | 能提取更细粒度和空间感知特征          |