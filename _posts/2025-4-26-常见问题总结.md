## 深度学习必备知识

### 数学知识

- 高等数学---泰勒拉格朗日
- 线性代数---特征值与矩阵分解
- 概率论---随机变量与概率估计
- 统计分析---回归分析与假设检验、相关分析

### 机器学习

**常用算法包括：**

* 分类算法：逻辑回归、决策树、向量机、集成算法、贝叶斯算法
* 回归算法：线性回归、决策树、集成算法
* 聚类算法：k-means算法 无监督算法
* 降维算法：主成分分析--降维用于减少数据特征数量的技术，旨在保留关键信息的同时降低计算复杂度、消除噪声或实现可视化

**常见算法实现：**

- 一元多元线性回归，非线性回归
- 逻辑回归
- K-means算法
- 决策树
- 贝叶斯算法
- 关联规则代码
- 音乐推荐系统
- 神经网络

---



### 目标检测

#### 传统目标检测

- 基于手工提取特征（选择特定区域进行特征提取，对特征进行检测分类）
  - 缺点：识别效果不好，精度不高；速度太慢

#### 基于深度学习目标检测算法

- 抽象为三个惯用组件Backbone(主干)、Neck(颈部)、Head(头部检测)
  - **Backbone**---特征提取
  - **Neck**--- 高低层特征的传递与融合（分而治之）
  - **Head**---最终预测

#### 常见问题

- **Faster-Rcnn网络原理**

`两阶段检测算法，利用RPN网络取代了选择性搜索算法使检测任务能有神经网络能端到端地完成：该网络将特征提取、候选区提取、边框回归和分类整合在一个网络当中；可以说网络模型主要分为两个部分RPN网络（区域提取网络）与RCNN(目标检测网络)`

- **简述R-CNN与faster R-CNN**

  ` RCNN主要是通过传统的选择搜索算法预取区域框，再将区域框统一尺寸输入CNN中提取特征，再将特征输入到支持向量机当中进行分类，对分类的区域框进行bbox回归`

  ---

  `fast R-CNN在R-CNN上的主要改进在于ROl特征图（使特征图尺寸一致利用最大池化等方式）提取将Rol特征输入到分类和回归模块中简化了R-CNN中的复杂算法过程`

  ---

  ` Faster R-CNN在前者的基础上利用RPN替代选择搜索算法极大提高了模型的精度与速度`

  ---

  `端到端的含义为直接从原始输入数据到最终输出结果的完整流程由单一模型完成，无需人工设计中间步骤或模块化子系统`

  

- **faster R-CNN是经典的two-stage 检测器，请简单说明其中two-stage的含义**

  `faster R-CNN之所以叫做 two-stage 检测器，原因是其包括一个区域提取网络 **RPN** 和 一个RoI Refine 网络 **R-CNN**(检测网络)，同时为了将 RPN 提取的不同大小的 RoI 特征图组成 batch 输入到后面的 R-CNN 中，在两者中间还插入了一个 RoI Pooling 层，可以保证任意大小特征图输入都可以变成指定大小输出。即在RPN提取ROI区域后，送入检测网络再进行分类和回归`

  ![faster-rcnn结构图示意图](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/faster-rcnn%E7%BB%93%E6%9E%84%E5%9B%BE%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg)

- **请简要说明一下RPN（Region Proposal Network）网络的作用**
  - RPN专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，成为一个整体
  - **RPN网络的实现细节:**
    - 1个特征图利用滑动窗口算法（利用固定尺寸窗口在数据中滑动遍历）（sliding window）处理，得到256维特征；对每个特征向量进行两次全连接操作
    - 会得到2K个分数（前景的分数和背景的分数）；4K个坐标（每个结果都有四个坐标及对原图的偏移）
    - 9个锚点，相当于特征图上有H×W×9个结果
  - **RPN的损失函数（二分类损失和SmoothL1损失）**
    - 定义二分类标签--是或不是
    - 锚点与锚点们与标注之间的最高重叠矩形区域或者锚点和标注的重叠区域指标（IOU）>0.7
    - ![RPN_loss](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/RPN_loss.png)
    - 偏移量损失 锚点框与原图标识框之间的偏移量
  - **为什么提出anchor box？**
    - 主要有两个原因：一个窗口只能检测一个目标、无法解决多尺度问题。目前anchor box尺寸的选择主要有三种方式： (1)人为经验选取 (2)k-means聚类 (3)作为超参数进行学习
  - **说一下RoI Pooling是怎么做的？有什么缺陷？有什么作用**
    - 池化最大的作用就在于统一固定大小的box矩形框
    - 1）根据输入image，将ROI映射到feature map对应位置 （2）将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）； （3）对每个sections进行max pooling操作；
    - **优点**： （1）允许我们对CNN中的feature map进行reuse； （2）可以显著加速training和testing速度； （3）允许end-to-end的形式训练目标检测系统
    - **缺点：**由于 RoIPooling 采用的是**最近邻插值（即INTER_NEAREST）** ，在resize时，对于缩放后坐标不能刚好为整数的情况，采用了粗暴的舍去小数，相当于选取离目标点最近的点，损失一定的**空间精度**
    - **两次整数化（量化）过程：** 1.region proposal的xywh通常是小数，但是为了方便操作会把它整数化。 2.将整数化后的边界区域平均分割成 k x k 个单元，对每一个单元边界进行整数化。
    - **ROI Align**相比**RoI Pooling**舍去了近似像素取整数的量化方法，改用**双线性插值**的方法确定特征图坐标对应于原图中的像素位置. ROI Align很好地解决了ROI Pooling操作中**两次量化**造成的区域不匹配(mis-alignment)的问题。
  
- **介绍faster rcnn这个流程，faster rcnn有哪些缺点？如何改进？**

  - **改进：**
    1. 更好的特征网络ResNet等；
    2. 更精确的RPN：可以使用FPN网络架构来设计RPN网络
    3. 更好的ROI分类方法：比如ROI分别在conv4和conv5上做ROI-Pooling，合并后再进行分类，这样基本不增加计算量，又能利用更高分辨率的conv4；
    4. 使用**Soft-NMS**代替NMS;（在目标检测任务中，**非极大值抑制（NMS） **是后处理阶段用于消除冗余检测框的关键步骤，但它存在一些固有缺陷（如硬性删除重叠框可能导致漏检）。**软性非极大值抑制（Soft-NMS）**通过改进NMS的抑制机制，能够更灵活地处理密集场景中的目标检测问题。以下是采用Soft-NMS代替传统NMS的主要理由及其技术细节：）

- **阐述一下Mask-RCNN网络，这个网络相比于Faster-RCNN网络有哪些改进的地方**

  - 强化基础网络：通过ResNext101+FPN模块作为特征提取网络

  - 利用ROI_Align替换ROI-Pooling，加了一个mask分支

  - 采用新的loss-function损失函数：损失函数使分类、回归再加上mask预测的损失之和![img](https://developer.qcloudimg.com/http-save/11431826/a1e85fab6c8fb89d7b45b3cf067c36f4.webp)

    

  - ![img](https://developer.qcloudimg.com/http-save/11431826/3ae6c8522babf591705b48cd2563ca3f.webp)

  - *Lcls*:分类损失；*Lbox*:边界框损失；*Lmask:*二值掩码损失


---

#### YOLO系列网络

- **YOLOV1、YOLOV2、YOLOV3复述一遍 YOLOv1到v3的发展历程以及解决的问题**

  ` YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。`

- **讲一下目标检测优化的方向**

  - 对数据集下手，提升特征表征强度
  - 从backbone下手，加深、加宽、换卷积方式、添加注意力机制，采取Tansformer在cv领域--Swim-Tramsformer
  - 从RPN下手（级联、FPN、IOU NET），**引入FPN结构，利用多层feature map融合来提高小目标检测的精度和召回**
  - LOSS损失函数

- **阐述一下如何进行多尺度训练多尺度训练可以分为两个方面:** **1. 图像金字塔 2. 特征金字塔**

  - **图像金字塔：** 人脸检测的MTCNN就是图像金字塔，使用多种分辨率的图像送到网络中识别，时间复杂度高，因为每幅图都要用多种scale去检测。 **特征金字塔：** FPN网络属于采用了特征金字塔的网络，一次特征提取产生多个feature map即一次图像输入完成，所以时间复杂度并不会增加多少
  
- **阐述如何检测小物体**

  - 小目标难以检测的原因：**分辨率低，图像模糊，携带的信息少**

  - 借鉴FPN的思想，在FPN之前目标检测的大多数方法都是和分类一样，使用顶层的特征来进行处理。虽然这种方法只是用到了高层的语义信息，但是位置信息却没有得到，尤其在检测目标的过程中，位置信息是特别重要的，而位置信息又是主要在网络的低层。因此FPN采用了多尺度特征融合的方式，采用不同特征层特征融合之后的结果来做预测。

  - 要让输入的分布尽可能地接近模型预训练的分布。先用ImageNet做预训练，之后使用原图上采样得到的图像来做微调，使用微调的模型来预测原图经过上采样的图像。该方法提升效果比较显著。

    采用多尺度输入训练方式来训练网络

    借鉴Cascade R-CNN的设计思路，优化目标检测中Two-Stage方法中的IOU阈值。检测中的IOU阈值对于样本的选取是至关重要的，如果IOU阈值过高，会导致正样本质量很高，但是数量会很少，会出现样本比例不平衡的影响；如果IOU阈值较低，样本数量就会增加，但是样本的质量也会下降。如何选取好的IOU，对于检测结果来说很重要；采用分割代替检测方法，先分割，后回归bbox来检测微小目标。

- **阐述目标检测任务中的多尺度**

  ``````
  输入图片的尺寸对检测模型的性能影响相当明显，事实上，多尺度是提升精度最明显的技巧之一。在基础网络部分常常会生成比原图小数十倍的特征图，导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。
  检测网络SSD中最后一层是由多个尺度的feature map一起组成的。FPN网络中采用多尺度feature map分层融合，分层预测的方法可以提升小目标的检测效果。
  - 阐述一下如何进行多尺度训练 多尺度训练可以分为两个方面: 
  1. 图像金字塔 2. 特征金字塔
  图像金字塔： 人脸检测的MTCNN就是图像金字塔，使用多种分辨率的图像送到网络中识别，时间复杂度高，因为每幅图都要用多种scale去检测。 特征金字塔： FPN网络属于采用了特征金字塔的网络，一次特征提取产生多个feature map即一次图像输入完成，所以时间复杂度并不会增加多少
  ``````

- **小目标不好检测的两大原因:**

  - 数据集中包含小目标的图片比较少，导致模型在训练的时候会偏向medium和large的目标
  - 小目标的面积太小了，导致包含目标的anchor比较少，这也意味着小目标被检测出的概率变小。

- **改进方法：**

  - 对于数据集中含有小目标图片较少的情况，使用过度采样（oversample）的方式，即多次训练这类样本。
  - 对于第二类问题，则是对于那些包含小物体的图像，将小物体在图片中复制多分，在保证不影响其他物体的基础上，人工增加小物体在图片中出现的次数，提升被anchor包含的概率。
  - 使用FPN（低层检测小目标）；
  - 对于分辨率很低的小目标，我们可以对其所在的proposal进行超分辨率，提升小目标的特征质量，更有利于小目标的检测

- **Yolo发展过程（4-25）**

  - Yolo-v1---核心思想就是将整张图作为网络的输入，直接在输出层回归边界框（bounding box)的位置以及其所属的类别

  - **YOLOv1的基本思想**: 把一副图片，首先reshape成448×448大小（由于网络中使用了全连接层，所以图片的尺寸需固定大小输入到CNN中），然后将划分成SxS个单元格（原文中S=7），以每个格子所在位置和对应内容为基础，来预测检测框和每个框的Confidence以及每个格子预测一共C个类别的概率分数。

  - **FPN的特征融合为什么是相加操作呢？**

    假设两路输入来说，如果是通道数相同且后面带卷积的话，add等价于concat之后对应通道共享同一个卷积核。FPN里的金字塔，是希望把分辨率最小但语义最强的特征图增加分辨率，从性质上是可以用add的。如果用concat，因为分辨率小的特征通道数更多，计算量是一笔不小的开销。所以FPN里特征融合使用相加操作可以理解为是为了降低计算量。

    **阐述一下FPN为什么能提升小目标的准确率?**

    低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。原来多数的object detection算法都是只采用顶层特征做预测。FPN同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同特征层的特征达到预测的效果。并且预测是在每个融合后的特征层上单独进行的。所以可以提升小目标的准确率。

    **基于FPN的RPN是怎么训练的？**

      在FPN的每个预测层上都接一个RPN子网，确定RPN子网的正负anchor box样本，再计算各预测层上RPN的anchor box分类和回归损失，利用BP将梯度回传更新权值

- **将检测问题转换成回归问题**

- **faster rcnn和yolo，ssd之间的区别和联系，分析一下SSD,YOLO,Faster rcnn等常用检测网络对小目标检测效果不好的原因**

  - 针对之前RCNN系列selective search的方法导致算法没有实时性，所以faster rcnn提出RPN网络来取代之前的方法，可以理解为fasterrcnn=fast rcnn+rpn网络，且rpn网络和fast rcnn的分类，回归网络共用特征提取层，这样使得引入RPN网络不会增加太多计算量。整体流程为先使用RPN网络找出可能存在object的区域，再将这些区域送入fast rcnn中进一步定位和分类。所以faster rcnn是典型的Two stage算法。因为faster rcnn中包含了两次定位，所以其精度一般高于YOLO和SSD算法，所以速度一般慢于YOLO和SSD

  - YOLO算法的特点是将检测问题转换成回归问题，即YOLO直接通过回归一次既产生坐标，又产生每种类别的概率。YOLO中将每张图分成7*7的网格，每个网格默认可能属于2个object，即在一张图片上提取98个region proposal，相比于faster rcnn使用Anchor机制提取20k个anchor再从中提取最终的300个region proposal，所以faster rcnn的精度比YOLO要高，但是由于需要处理更多region proposal，所以faster rcnn的速度要比YOLO慢。

  - SSD相比于faster rcnn使用了多层网络特征，而不仅仅使用最后一层feature map。SSD还借鉴了YOLO算法中将检测任务转换为回归任务的思想，且SSD也借鉴了faster rcnn中的anchor机制，只是SSD的anchor不是每个位置的精调，而是类似于YOLO那样在feature map上分割出网格，在网格上产生anchor。但是SSD和YOLO不需要selective search步骤，所以SSD和YOLO同属于One-Stage算法。

  - **SSD，YOLO**等单阶段多尺度算法，小目标检测需要较高的分辨率，SSD对于高分辨的低层特征没有再利用，而这些层对于检测小目标很重要。按SSD的设计思想，其实SSD对小目标应该有比较好的效果，但是需要重新精细设计SSD中的default box，比如重新设计min_sizes参数，扩大小default box的数量来cover住小目标。但是随着default box数量的增加，网络速度也会降低。YOLO网络可以理解为是强行把图片分割成7*7个网格，每个网格预测2个目标，相当于只有98个anchor，所以不管是小目标，还是大目标，YOLO的表现都不是很理想，但是由于只需处理少量的anchor，所以YOLO的速度上有很大优势。

    **Faster rcnn**系列对小目标检测效果不好的原因是faster rcnn只用卷积网络的最后一层，但是卷积网络的最后一层往往feature map太小，导致之后的检测和回归无法满足要求。甚至一些小目标在最后的卷积层上直接没有特征点了。所以导致faster rcnn对小目标检测表现较差。

- **请列举目标检测中常见的Loss**

  - **分类损失：**
    - **交叉熵损失函数**$$ L = -ylog(p)-(1-y)log(1-p)  $$
    - **focal loss**出于论文Focal Loss for Dense Object Detection，主要是为了解决one-stage目标检测算法中正负样本比例严重失衡的问题，降低了大量简单负样本在训练中所占的比重，可理解为是一种困难样本挖掘。focal loss是在交叉熵损失函数上修改的。具体改进$$ FL(p, y)=-\alpha y (1-p)^{\gamma}log(p)-(1-\alpha)(1-y)p^{\gamma}log(1-p)  $$  focal loss出来之后，不少loss基于focal loss延伸

  - **回归损失：**

- **focal loss解决什么问题，如何写，每个参数有什么作用?**

  - Focal loss主要是为了解决**one-stage**目标检测中**正负样本比例严重失衡**的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘 $$FL(p, y)=- y (1-p)^{\gamma}log(p)-(1-y)p^{\gamma}log(1-p) \= {\begin{array}{ll}- (1-p)^{\gamma}log(p) & y=1 \-p^{\gamma}log(1-p) & y=0\end{array} $$ 添加参数γ，当γ大于0时，对于易分的正样本或负样本，权重小，而对于难区分的样本则权重大，避免让简单样本主导loss，γ越大，困难样本的权重越大 $$ FL(p, y)=-\alpha y (1-p)^{\gamma}log(p)-(1-\alpha)(1-y)p^{\gamma}log(1-p) \= {\begin{array}{ll}-\alpha (1-p)^{\gamma}log(p) & y=1 \-(1-\alpha)p^{\gamma}log(1-p) & y=0\end{array} $$ 添加参数α，用来平衡正负样本本身的比例不均

- **训练过程中loss一致无法收敛，可能的原因**

  - **数据和标签** 数据分类标注是否准确？数据是否干净？数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间。样本少只可能带来过拟合的问题
  - **学习率设定不合理** 在自己训练新网络时，可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。不过刚刚开始不建议把学习率设置过小，尤其是在训练的开始阶段。在开始阶段我们不能把学习率设置的太低否则loss不会收敛。我的做法是逐渐尝试，从0.1,0.08,0.06,0.05 ......逐渐减小直到正常为止，
  - **网络设定不合理** 如果做很复杂的分类任务，却只用了很浅的网络，可能会导致训练难以收敛，换网络换网络换网络，重要的事情说三遍，或者也可以尝试加深当前网络。
  - **数据集label的设置** 检查lable是否有错，有的时候图像类别的label设置成1，2，3正确设置应该为0,1,2。
  - **改变图片大小** 改变图片大小有时可以解决收敛问题
  - **数据归一化** 神经网络中对数据进行归一化是不可忽略的步骤，网络能不能正常工作，还得看你有没有做归一化，一般来讲，归一化就是减去数据平均值除以标准差，通常是针对每个输入和输出特征进行归一化

- **kmeans算法的内容**

  - k-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。 **K-means算法原理:** K-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。算法详细的流程描述如下：
  - ![img](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/kmeans%E7%AE%97%E6%B3%95.jpg)
  - **k-means算法优缺点分析:**
    - 优点： 算法简单易实现；
    - 缺点： 需要用户事先指定类簇个数； 聚类结果对初始类簇中心的选取较为敏感； 容易陷入局部最优； 只能发现球形类簇；

- **非极大值抑制（NMS）（non maximum suppression） NMS实现细节**

  - **用处:** 本质是搜索局部极大值，抑制非极大值元素。

  - **原理:** NMS为非极大值抑制，用来抑制检测时冗余的框。

  - **大致算法流程为：** 1.对所有预测框的置信度降序排序 2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的IOU 3.根据2中计算的IOU去除重叠度高的，IOU>threshold阈值就删除 4.剩下的预测框返回第1步，直到没有剩下的为止

    （*需要注意的是：Non-Maximum Suppression一次处理一个类别，如果有N个类别，Non-Maximum Suppression就需要执行N次*）

  - **假设两个目标靠的很近，则会识别成一个bbox，会有什么问题，怎么解决？**

    当两个目标靠的非常近时，置信度低的会被置信度高的框抑制掉，从而两个目标靠的非常近时会被识别成一个bbox。为了解决这个问题，可以使用soft-NMS（基本思想：用稍低一点的分数来代替原有的分数，而不是直接置零）


---

### CV基础知识（4-22）

- **为什么需要做特征归一化、标准化？**
  - 使不同量纲的特征处于统一数值量级，**减少方差大的特征的影响**，使模型更准确
  - 加快学习算法的收敛速度（**归一化、标准化和中心化/零均值化**）
- **常用的归一化和标准化的方法**
  - 线性归一化 $x=\frac{(x-min(x))}{(max(x)-min(x))}$其中max是样本数据的最大值，min是样本数据的最小值，适用于数值比较集中的情况，可使用的经验值常量来代替max,min。
  - 标准差归一化$x=\frac{x-\mu}{\sigma}$其中$\mu$为所有样本的均值，$\sigma$为所有样本的标准差，经过处理后符合标准正态分布均值为0，标准差为1。
  - 非线性归一化，使用非线性函数log,指数、正切，一般用于数据分化比较大的场景。
- **介绍空洞卷积的原理和作用**
  - 空洞卷积也称膨胀卷积，最初是为了解决图像分割在用**下采样**（池化，卷积）**增加感受野**是带来的**特征图的缩小**，后面再采用**上采样**回去时造成的精度上的损失。
  - **空洞卷积**通过引入一个扩张率的超参数，该参数定义了卷积核处理数据时每个值的间距，**可以再增加感受野的同时保持特征图的尺寸不变**，从而代替下采样和上采用，通过**调整扩张率**得到不同的感受野大小
    - 扩张率为1时卷积后的感受野为3
    - 扩张率为2的空洞卷积，卷积后的感受野为5
    - 扩张率为3的空洞卷积，卷积后的感受野为8 
    - **普通卷积时空洞卷积的一种特殊情况**
- **怎么判断模型是否过拟合，有哪些防止过拟合的策略？**
  - 当模型在训练集的精度很高，而在测试集上的精度很差时模型过拟合；当模型在测试集和训练集上精度都很差时模型欠拟合。
  - **预防过拟合的策略**
    - 增加训练数据，利用数据增强等 从数据源头出发；
    - 使用合适训练模型，从模型出发；减少网络层数，降低网络参数量；
    - Dropout随机抑制神经网络当中的一部分神经元使得每次训练时都有一部分神经元不参与模型训练
    - L1、L2正则化，训练时限制权重大小，增加惩罚机制是网络更加稀疏；
    - 数据清洗：去除问题数据，错误标签和噪声数据；
    - 限制网络训练时间
    - 网络中适当添加BN批归一化层能有效防止过拟合的出现；
- **优化算法**
  - 主要有三大类：
    a. 基本梯度下降法，包括 GD，BGD，SGD；
    b. 动量优化法，包括 Momentum，NAG 等；
    c. 自适应学习率优化法，包括 Adam，AdaGrad，RMSProp 等
  - 优化算法公式：$w=w+\delta$ 各种优化算法的主要区别在于对$\delta$​的计算不同
  - SGD Momentum Nesterov Momentum AdaGrad RMSProp AdaDelta Adam AdaMax Nadam NadaMax
- **阐述感受野的概念，并说一下在CNN中如何计算**
  - 感受野指的是卷积神经网络每一层输出的特征图上每个像素点映射回输入图像上的区域大小；神经元感受野的范围越大表示器接触到的原始图像范围越大，也就意味着它能学习的更加全局，语义曾是更高的特征信息。易得到当网络越深时，神经元的感受野越大。而卷积时的感受野也与该层的卷积核尺寸和步长有关。于padding周长外围无关。

---

#### 4-23

- **神经网络的深度和宽度的含义**
  - 深度代表了网络的表达能力，早期主干backbone主要是**直接堆叠卷积层**，它的深度代表了神经网络的层数；后面主干采用为更加**高效的module（block）堆叠的方式**，此时深度指的是module的个数。
  - 宽度决定了网络在某层学习到的信息量，指的是卷积神经网络中最大的通道数，由卷积核数量最多的层决定。**通常的结构设计中卷积核的数量随着层数越来越多的，直到最后一层特征图（feature map）达到最大**，因为越到深层，特征图的分辨率越小，所包含的信息越高级，所以需要更多的卷积核来进行学习；**通道数越多越好，但是带来的计算量也会大大增加**。
  
- **上采样的原理和常用方式**
  - 定义为使图像由小分辨率映射到大分辨率的操作
    - 插值---双线性插值还有最近领插值，三线性插值
    - 转置卷积---通过对输入特征图间隔填充0，在进行标准的卷积计算，可以使得输出的特征图的尺寸比输入更大。
    - Max-Upooling，在对称的max pooling位置记录最大值的索引位置，然后再unpooling阶段时将对应的值防止再原先最大值位置其余为0
  - https://zhuanlan.zhihu.com/p/344354520
  
- **下采样的作用，哪些方式**
  - 作用：减少计算量，防止过拟合；增加感受野，使后面的卷积核能学到更加全局的信息
  - 方式：1、采用步长为2的池化层，目前的Max-pooling,计算简单更好的保存纹理特征；2、步长为2的卷积层，下采样是一个信息损失的过程，而池化层是不可学习的用步长为2的可学习卷积层代替池化层能得到更好的效果。但计算量会增加。
  
- **模型的参数量和计算方法**
  - 参数量值的是网络中可学习变量的数量，包含卷积核的权重weights,批归一化BN的缩放系数和偏移系数--都是可学习的参数；记载模型训练开始前就被赋予初值再训练过程根据链式法则不断迭代更新，**整个模型的参数量主要是由卷积核的权重的数量决定**，参数量越大，对内存要求也越高。
  - Kh x Kw x Cin xCout //高宽 输入通道数 输出通道数 
  - Cin x Cout//FC全连接网络
  - **计算量=输出的特征图*当前层filter（H × W × Cout） × （K × K × Cin）**
  
- **深度可分离卷积的概念核作用**
  - 深度可分离卷积将传统卷积分两步进行分别是depthwise和pointwise；首先按照通道进行计算按位相乘的计算，深度可分离卷积的卷积核都是单通道的，输出不能改变特征图的通道数，此时通道数不变。然后依然得到将第一步的结果，使用1x1的卷积核进行传统的卷积运行，此时通道数可以改变。
  - **计算量的前后对比：**
    Kh × Kw × Cin × Cout × H × W 变成了 Kh × Kw × Cin × H × W + 1 × 1 × Cin × Cout × H × W
  
  ****

- **神经网络中的add和concate的区别**

  - Add在ResNet中提出，两个相同维度的特征图相同位置点的值直接相加，得到新的相同维度特征图，这个操作可以融合之前的特征，增加信息的表达
  - Concate操作在Inception提出，区别在于他只要求两个特征图的高宽（HW）相同，通道数可以不同，然后两个特征图在通道上进行直接拼接，得到一个更大的特征图，他保留了一些原始的特征，增加了特征的数量。![3e1f4862466b01fdfb642f6c1d6da19b.png](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/3e1f4862466b01fdfb642f6c1d6da19b.png)

- **常见的激活函数**

  - 定义：激活函数是模型整个结构中的非线性扭曲力，神经网络的每层都会有一个激活函数
  - 常用激活函数：Sigmoid函数、tanh函数、Relu函数、Leaky ReLU函数、MaxOut函数
  - [常用的激活函数](https://www.cnblogs.com/wj-1314/p/12015278.html)
  - [常用激活函数的比较](https://zhuanlan.zhihu.com/p/32610035)

- **神经网络中1×1卷积有什么作用**

  - 降维，减少计算量；在ResNet模块中，先通过1×1卷积对通道数进行降通道，再送入3×3的卷积中，能够有效的减少神经网络的参数量和计算量；
  - 升维；用最少的参数拓宽网络通道，通常在轻量级的网络中会用到，经过深度可分离卷积后，使用1×1卷积核增加通道的数量，例如mobilenet、shufflenet等；
  - 实现跨通道的交互和信息整合；增强通道层面上特征融合的信息，在feature map尺度不变的情况下，实现通道升维、降维操作其实就是通道间信息的线性组合变化，也就是通道的信息交互整合的过程；
  - 1×1卷积核可以在保持feature map尺度（不损失分辨率）不变的情况下，大幅增加非线性特性（利用后接的非线性激活函数）

- **随机梯度下降相比全局梯度下降好处是什么？**

  - 当处理大量数据时，比如SSD或者faster-rcnn等目标检测算法，每个样本都有大量候选框参与训练，这时使用随机梯度下降法能够加快梯度的计算；
  - 每次只随机选取一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新

- **如果在网络初始化时给网络赋予0的权重，这个网络能正常训练嘛**

  - 不能，因为初始化权重是0，每次传入的不同数据得到的结果是相同的。网络无法更新

- **梯度消失和梯度爆炸的原因是什么？**

  - **原因**：激活函数的选择
  - **梯度消失**：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较小时，则权重参数呈指数级减小。
  - **梯度爆炸**：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较大时，则权重参数呈指数级增长。

- **为什么神经网络种常用relu作为激活函数**

  - 在前向传播和反向传播过程中，ReLU相比于Sigmoid等激活函数计算量小；
  - 在反向传播过程中，Sigmoid函数存在饱和区，若激活值进入饱和区，则其梯度更新值非常小，导致出现梯度消失的现象。而ReLU没有饱和区，可避免此问题；
  - ReLU可令部分神经元输出为0，造成网络的稀疏性，减少前后层参数对当前层参数的影响，提升了模型的泛化性能

- **卷积层和全连接层的区别是什么**

  - 卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息；
  - 当卷积层的局部连接是全局连接时，全连接层是卷积层的特例；

- **什么是正则化？L1正则化和L2正则化有什么区别？**

  所谓的正则化，就是在原来 Loss Function 的基础上，加了一些正则化项，或者叫做模型复杂度惩罚项，正则化机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。

  - **两者的区别:**
    - L1：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”
    - L2：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了过拟合。不过岭回归并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。
      L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重

- **常见的损失函数有哪些？你用过哪些？**

  - 平方损失（预测问题）、交叉熵（分类问题）、hinge损失（SVM支持向量机）、CART回归树的残差损失
  - [深度学习损失函数大全](https://mp.weixin.qq.com/s/jYvTvA_LZjJrDQ2q6Crg3w) 

- **数据不平衡的解决方法**

  - 欠采样--随机删除观测足够多的类，使得两个类别间的相对比列使显著的。
  - 过采样--对于不平衡的类别，我们使用拷贝现有样本的方法随机增加观测数量。理想情况下样本平衡，若采用过采样会造成过拟合；
  - 合成采样--利用合成方法得到不平衡类别的观测，这技术与现有的使用最近邻分类方法类似。问题在于当一个类别的观测数量极其稀少时使用。
  - 可以采用focal loss等loss进行控制不平衡样本

- **ReLU函数在0处不可导，为什么还能用？**

  - 反馈神经网络正常工作需要的条件就是每一个点提供一个方向，即导数；0值不可微，本质上来说是因为这个地方可画多条切线，但我们需要的只是一条；由于这出现的0值的概率极低，任意选择一个子梯度就OK了，在0处的次微分集合是【0，1】；即选择其中一个就OK了；一般默认是0；

- **Pooling层的作用以及如何进行反向传播**

  池化层没有可以训练的参数，因此在训练中，池化层只需要将误差传递到上一层，并不需要进行梯度的计算。梯度之和不变；

  - **平均池化（average pooling）**：前向传播是取某特征区域的平均值进行输出，这个区域的每一个神经元都是有参与前向传播了的，因此，在反向传播时，框架需要将梯度平均分配给每一个神经元再进行反向传播;![KocaU1zbxnXYsyJ.jpg](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/KocaU1zbxnXYsyJ.jpg)
  - **最大池化（Max pooling）:**前向传播是取某特征区域的最大值进行输出，这个区域仅有最大值神经元参与了前向传播，因此，在反向传播时，框架仅需要将该区域的梯度直接分配到最大值神经元即可，其他神经元的梯度被分配为0且是被舍弃不参与反向传播的，但如何确认最大值神经元，这个还得框架在进行前向传播时记录下最大值神经元的Max ID位置。![CZnUSwEcFy84JVL.jpg](https://github.com/datawhalechina/daily-interview/raw/master/AI%E7%AE%97%E6%B3%95/CV/images/CZnUSwEcFy84JVL.jpg)

  - [池化层的反向传播的实现](https://blog.csdn.net/Jason_yyz/article/details/80003271)

- **为什么max pooling 要更常用？什么场景下 average pooling 比 max pooling 更合适？**
  - 作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。
  - 通常来讲，*max-pooling*的效果更好，虽然*max-pooling*和*average-pooling*都对数据做了下采样，但是*max-pooling*感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。 *pooling*的主要作用一方面是**去掉冗余信息**，一方面要**保留feature map的特征信息**，在分类问题中，我们需要知道的是这张图像有什么*object*，而**不大关心这个object位置**在哪，在这种情况下显然*max pooling*比*average pooling*更合适。在网络比较深的地方，特征已经稀疏了，从一块区域里选出最大的，比起这片区域的平均值来，更能把稀疏的特征传递下去。
  - *average-pooling*更强调对**整体特征信息**进行一层下采样，在**减少参数维度**的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说*DenseNet*中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。
  - *average-pooling*在全局平均池化操作中应用也比较广，在*ResNet*和*Inception*结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替*Flatten*操作，使输入数据变成一维向量。*Flatten*是一种特殊的reshape操作，其中所有的轴都被平滑或压扁在一起。[Flatten介绍](https://blog.csdn.net/flyfor2013/article/details/105721695?ops_request_misc=%257B%2522request%255Fid%2522%253A%25227fa4577d63fd754b4d87be6bd3cf147d%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=7fa4577d63fd754b4d87be6bd3cf147d&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105721695-null-null.142^v102^pc_search_result_base1&utm_term=Flatten%E6%93%8D%E4%BD%9C&spm=1018.2226.3001.4187)

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e6ab0ec392c65883dc69cade4cb4042b.jpeg#pic_center)![img](https://i-blog.csdnimg.cn/blog_migrate/f454d50cfbb86fe601449934019dc05f.png)![img](https://i-blog.csdnimg.cn/blog_migrate/2e7ab0ace10f23b59f8bc861487b4e9e.png)



- **为什么要反向传播？手推反向传播公式展示一下**
  - 反向传播算法的motivation是期望通过在神经网络的训练过程中自适应的调整各神经元间的连接权值，以寻求最佳的输入输出间的映射函数，使得目标函数或损失函数达到最小，完成分类、回归等任务[BP（反向传播算法）公式推导及例题解析](https://zhuanlan.zhihu.com/p/32819991)
- **有什么数据增强的方式**
  - **单样本几何变换**：翻转，旋转，裁剪，缩放
  - **单样本像素内容变换**：噪声，模糊，颜色扰动
  - **多样本插值 Mixup**：图像和标签都进行线性插值
- **模型训练开始会有warm up**
  - warm up, 在刚刚开始训练时以很小的学习率进行训练，使得网络熟悉数据，随着训练的进行学习率慢慢变大，到了一定程度，以设置的初始学习率进行训练，接着过了一些inter后，学习率再慢慢变小；学习率变化：**上升——平稳——下降。**
    - 有助于减缓模型在初始化阶段对mini-batch的提前过拟合现象，保持分布的平稳；
    - 有助于保持模型深层的稳定性；
- **分组卷积Group Convolution**
  - 若卷积神经网络的上一层有N个卷积核，则对应的通道数也为N。设**群数目为M**，在进行卷积操作的时候，将通道分成M等分，**每个group对应N/M个通道数**，然后每个group卷积完成后输出叠在一起，作为当前层的输出通道数；

- **训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些**
  - 并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作
- **Relu比Sigmoid的效果好在哪里?**
  - Sigmoid的导数只有**在0的附近时有较好的激活性**,而在正负饱和区域的梯度趋向于0,从而产生**梯度弥散**的现象,而relu在**大于0的部分梯度为常数**,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此**神经元不参与训练,具有稀疏性**。

- **批归一化Batch Normalization的作用**

  - 神经网络在训练的时候随着网络层数的加深,**激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近**,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将**越来越偏的分布拉回到标准化的分布**,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,**避免梯度消失**的问题。

- **GAN网络的思想**

  **`GAN用一个生成模型和一个判别模型,判别模型用于判断给定的图片是不是真实的图片,生成模型自己生成一张图片和想要的图片很像,开始时两个模型都没有训练,然后两个模型一起进行对抗训练,生成模型产生图片去欺骗判别模型,判别模型去判别真假,最终两个模型在训练过程中,能力越来越强最终达到稳态。`**

- **Attention机制的作用**
  - 减少处理高维输入数据的计算负担,**结构化**的选取输入的子集,从而**降低数据的维度**。让系统更加容易的找到输入的数据中与当前输出信息相关的**有用信息,**从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。
- **怎么提升网络的泛化能力**
  - **从数据上提升性能**:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。 从**算法调优**上提升性能:用可靠的模型诊断工具对模型进行诊断,**权重的初始化**,用小的随机数初始化权重。对**学习率**进行调节,尝试**选择合适的激活函数**,**调整网络的拓扑结构**,**调节batch和epoch的大小**,**添加正则化**的方法,尝试使用其它的优化方法,使用early stopping

---

#### BERT的基本原理（4-25）

BERT 是“Bidirectional Encoder Representations from Transformers”的首字母缩写，整体是一个自编码语言模型（Autoencoder LM），并且其设计了两个任务来预训练该模型。

- 第一个任务是采用 MaskLM 的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据上下文去学习这些地方该填的词。

- 第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。
- BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合

- **BERT的输入和输出**

  - BERT 模型的主要输入是文本中各个字/词(或者称为 token)的原始词向量，该向量既可以随机初始化，也可以利用word2vec等算法进行预训练以作为初始值；输出是文本中各个字/词融合了全文语义信息后的向量表示。
  - **文本向量(英文中对应的是 Segment Embeddings)：**该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/词的语义信息相融合
  - **位置向量(英文中对应的是 Position Embeddings)：**由于出现在文本不同位置的字/词所携带的语义信息存在差异（比如：“我爱你”和“你爱我”），因此，BERT 模型对不同位置的字/词分别附加一个不同的向量以作区分

  最后，BERT 模型将**字向量**、**文本向量**和**位置向量**的加和作为模型输入。特别地，在目前的 BERT 模型中，文章作者还将英文词汇作进一步切割，划分为**更细粒度**的语义单位（WordPiece），例如：将 playing 分割为 play 和##ing；此外，对于中文，目前作者未对输入文本进行分词，而是直接将**单字**作为构成文本的基本单位。

​	

- **BERT中的Transformer构成**
  - 需要注意的是，与Transformer本身的Encoder端相比，BERT的Transformer Encoder端输入的向量表示，多了**Segment Embeddings**
- **BERT 的三个 Embedding 直接相加会对语义有影响吗**
  - **One-Hot编码**是一种将**分类变量**转换**二进制向量**的方法，基本思想就是将每个类别表示为一个二进制向量，向量的长度等于类别总数，其中只有一个元素为1，其余元素为0.
  - **原理和作用：**将**离散**的分类特征转换为机器学习模型可以处理的数值形式。在机器学习中，特征通常是连续的数值，而分类特征（如性别、国家等）通常是离散的。One-Hot编码通过**为每个类别创建一个二进制向量，使得这些离散特征能够被模型识别和处理‌**
  - Embedding 的**数学本质**，就是以 **One-Hot** 为输入的单层全连接;现在将token、position、segment三者都用**One-Hot**表示，然后**concat**起来，然后才去过一个单层全连接，等价的效果就是三个Embedding相加。
  - **因此，BERT 的三个 Embedding 相加，其实可以理解为 token, position, segment 三个用 one hot 表示的特征的 concat**
- **BERT 的MASK方式的优缺点**
  - BERT的**mask方式**：在选择mask的15%的词当中，80%情况下使用mask掉这个词，10%情况下采用一个任意词替换，剩余10%情况下保持原词汇不变；**用于在训练过程中随机遮蔽输入文本的一部分单词，然后模型尝试预测这些被遮蔽的单词**；
  - 1、双向上下文理解；2、提高模型泛化能力；3、增强模型鲁棒性；
  - 1、训练复杂度增加；2、预测难度增加；3、可能引入偏差；4、推理阶段的局限性
    - 被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力
    - 被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）
    - 针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息
- **BERT深度双向的特点**
  - 预训练模型中，预训练任务是一个Mask LM，随机的把句子的单词替换成mask标签，然后对单词进行预测；对于模型，输入的是一个**被挖了空的句子**， 而由于**Transformer**的特性， 它是会注意到所有的单词的，这就导致模型会根据挖空的上下文来进行预测， 这就实现了双向表示， 说明BERT是一个双向的语言模型。
  - BERT使用**Transformer-encoder**来编码输入，**encoder**中的**Self-attention**机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。
- **BERT深度双向的特点**
  - 针对特征提取器，**Transformer**只用了**self-attention**，没有使用RNN、CNN，并且使用了**残差连接**有效防止了**梯度消失**的问题，使之可以构建更深层的网络，所以BERT构建了多层深度Transformer来提高模型性能
- **BERT的embedding向量**
  - BERT 模型通过**查询字向量表**将文本中的每个字转换为一维向量，作为模型输入(还有 **position embedding** 和 **segment embedding**)；模型输出则是输入各字对应的融合**全文语义信息**后的向量表示。
  - 对于输入的 **token embedding**、**segment embedding**、**position embedding** 都是随机生成的，需要注意的是**在 Transformer 论文中的 position embedding 由 sin/cos 函数生成的固定的值**，而在这里代码实现中是跟普通 word embedding 一样随机生成的，可以训练的。
- **BERT中Transformer中的Q、K、V存在的意义**
  - 在使用**self-attention**通过上下文词语计算当前词特征的时候，X先通过WQ、WK、WV线性变换为QKV，然后使用QK计算得分，最后与V计算加权和而得
  - 倘若不变换为QKV，直接使用每个token的向量表示点积计算重要性得分，那在**softmax后的加权平均**中，该词本身所占的比重将会是最大的，使得其他词的比重很少，无法有效利用**上下文信息**来增强当前词的语义表示。而变换为QKV再进行计算，能有效利用上下文信息，很大程度上减轻上述的影响。
- **BERT中Transformer中Self-attention后为什么要加前馈网络**
  - 使用多头注意力，能够从**不同角度**提取信息，提高**信息提取**的全面性
  - 由于**self-attention**中的计算都是线性了，为了提高模型的**非线性拟合能力**，需要在其后接上前馈网络
- **BERT参数量计算**
  - bert的参数主要可以分为四部分：**embedding层的权重矩阵**、**multi-head attention**、**layer normalization**、**feed forward**。
  - **embedding**层由**token embedding**、**segment embedding**和**position embedding**构成
  - **multi-head attention多头注意力机制：**Q，K，V就是我们输入的三个句子词向量，从之前的词向量分析可知，输出向量大小从len -> len x hidden_size，即len x 768。如果是self-attention，Q=K=V，如果是普通的attention，Q !=K=V。但是，不管用的是self-attention还是普通的attention，参数计算并不影响。因为在输入单头head时，对QKV的向量均进行了不同的线性变换，引入了三个参数，W1，W2，W3。其维度均为：768 x 64。
  - **layer normalization:** embedding层后、multi-head attention后、feed forward后
  -  **feed forward全连接层**

---

#### Yolo的常见疑问

- **运行后的精度是模型再测试集上的精度吗？**
  - **训练集**：用于训练模型权重
  - **验证集**：在训练过程中评估模型性能、调整超参数（如学习率）和防止过拟合（例如通过早停机制）。YOLOv8 默认会在每个 epoch 后输出验证集的指标（如 `mAP50-95`、`precision`、`recall`）
  - **测试集**：仅在模型完全训练完成后使用，用于最终独立评估模型性能，反映其泛化能力。它不参与任何训练或调参。
  - 训练时，YOLOv8 会**自动划分数据**（默认比例为 `train:val=80%:20%`），或在数据配置文件（如 `data.yaml`）中按用户指定的路径加载验证集
  - 训练日志（如 `results.csv`）中的精度指标（如 `metrics/mAP50-95(B)`）均来自验证集

``````
yolo val model=path/to/best.pt data=data.yaml split=test
#或者采用python
from ultralytics import YOLO
model = YOLO('path/to/best.pt')
model.val(data='data.yaml', split='test')
``````

- **在自制数据集时是直接进行标注还是统一尺寸后标注？**
  - 优先考虑直接标注
    - **保留原始信息**：不压缩或拉伸图片，避免因缩放导致小目标失真或标注位置偏差。
    - **标注效率高**：省去预处理步骤，直接标注更快速
    - **灵活性**：后期训练时可通过数据增强（如随机缩放、裁剪）统一输入尺寸。
