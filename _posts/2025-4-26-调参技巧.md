### 调参技巧

#### 学习率

---

**越大的batch-size使用越大的学习率**，越大的batch-size意味着学习时，收敛方向的confidence越大，前进方向更加坚定；而小的batch-size会显得加乱毫无规律性，因为相比批次大的时候，批次小的情况无法照顾到更多的情况，所以需要小的学习率来保证不出错。



#### 权重初始化

---

前提时只有在没有预训练模型的领域才会要自己初始化权重，或者在模型中去初始化神经网络最后那几个全连接层的权重。常见的初始化算法有kaiming_normal和xavier_normal

#### Dropout

---

dropout指在深度学习网络的训练过程中，对于神经网络单元按照一定的概率将其暂时从网络中丢弃。注意是「**暂时**」，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。Dropout类似于bagging ensemble减少variance。也就是投通过投票来减少可变性。通常我们在**全连接层**部分使用dropout，在卷积层则不使用。但「dropout」并不适合所有的情况，不要无脑上`Dropout`。`Dropout`一般适合于全连接层部分，而卷积层由于其参数并不是很多，所以不需要dropout，加上的话对模型的泛化能力并没有太大的影响。![图片](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qGkrv1x6Zkib5byzxNF8m8T2Zv65KgpBbayhVPyH07uqNBV7JR0o0kOUtbUibtCluAdQk5lDwhBDWicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

我们一般在网络的最开始和结束的时候使用全连接层，而hidden layers则是网络中的卷积层。

#### 多模型融合

---

- 同样的参数,不同的初始化方式
- 不同的参数,通过cross-validation,选取最好的几组
- 同样的参数,模型训练的不同阶段，即不同迭代次数的模型
- 不同的模型,进行线性融合. 例如RNN和传统模型

提高模型性能和鲁棒性大法：**probs融合** 和 **投票法**

- 模型 1 问题 + 模型 2 问题 + 模型 3 问题 ==%3A 最终标签
- model1 标签 ， model2 标签 ， model3 标签 ==> 投票 ==> 最终标签
- model1_1 probs + ... + model1_n probs ==> mode1 label， model2 label 与 model3 获取的 label 方式与 1 相同 ==> voting ==> final label
- 如果一个model的随机种子没有固定，多次预测得到的结果可能不同。



#### 余弦退火和热重启的随机梯度下降

---

**余弦**就是类似余弦函数的曲线，**退火**就是下降，可知**余弦退火**就是学习率类似余弦函数慢慢下降。**热重启**则是学习过程中，学习率慢慢下降然后突然回弹让后在继续慢慢下降。![图片](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qGkrv1x6Zkib5byzxNF8m8T28jjqkXJ8KDKJ0TLBdSuyhdke1ZEPaS2HAHnyOLdtRMiazX2a2K5vnpw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

#### 差分学习率与迁移学习

---

迁移学习是一种常见的深度学习技巧，利用很多预训练的经典模型直接去训练我们自己的任务。虽说领域不同，但是在学习权重的广度方面。两个人物之间还是有联系的。![图片](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qGkrv1x6Zkib5byzxNF8m8T2Bq2C7qHuQ6o5UXBGhReS97XozLk6wVcUj5pHF2vQklZpCiaYViaoRQwQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

**用训练好的model A的模型权重去训练我们自己模型model B的权重,其中，modelA可能是ImageNet的预训练权重，而ModelB则是想要用来识别猫狗的预训练权重。**我们设计的神经网络(如下图)一般分为三个部分，输入层，隐含层和输出层，随着层数的增加，神经网络学习到的特征越抽象。因此，下图中的卷积层和全连接层的学习率也应该设置的不一样，一般来说，卷积层设置的学习率应该更低一些，而全连接层的学习率可以适当提高。**这就是差分学习率的意思，在不同的层设置不同的学习率，可以提高神经网络的训练效果，具体的介绍可以查看下方的连接。**



#### 多尺度训练

---

多尺度训练是一种直接有效的方法，通过**不同尺寸的图像数据集**，因为神经网络卷积池化的特殊性，这样可以让神经网络充分地学习不同分辨率下图像的特征，可以提高机器学习的性能。也可以用来**处理过拟合效应**，在图像数据集不是很充足的情况下，可以**先**训练小尺寸图像，然**后**增大尺寸并再次训练相同模型。

需要注意的是：多尺度训练并不是适合所有的深度学习应用，多尺度训练可以算是特殊的数据增强方法，在图像大小这一块做了调整。如果有可能最好利用可视化代码将多尺度后的图像近距离观察一下，「看看多尺度会对图像的整体信息有没有影响」，如果对图像信息有影响的话，这样直接训练的话会误导算法导致得不到应有的结果



#### 交叉验证

---

在李航的统计学方法中说到，交叉验证往往是对实际应用中「**数据不充足**」而采用的，基本目的就是重复使用数据。在平常中我们将所有的数据分为训练集和验证集就已经是简单的交叉验证了，可以称为**1折交叉验证**。「注意，交叉验证和**测试集**没关系，测试集是用来衡量我们的算法标准的，不参与到交叉验证中来。」交叉验证只针对训练集和验证集。交叉验证是Kaggle比赛中特别推崇的一种技巧，我们经常使用的是**5-折(5-fold)交叉验证**，将训练集分成5份，随机挑一份做验证集其余为训练集，循环5次，这种比较常见计算量也不是很大。还有一种叫做`leave-one-out cross validation`留一交叉验证，这种交叉验证就是n-折交叉，n表示数据集的容量，这种方法只适合**数据量比较小**的情况，计算量非常大的情况很少用到这种方法。![图片](https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qGkrv1x6Zkib5byzxNF8m8T2FibNzWD7YHYQ7qfhibxVlApciaWWSGibbDpb5xEjEIOpI4zUicU6kt0VDJQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)

#### 优化算法

---

按理说不同的优化算法适合于不同的任务，不过我们大多数采用的优化算法还是是adam和SGD+monmentum。**Adam 可以解决一堆奇奇怪怪的问题**（有时 loss 降不下去，换 Adam 瞬间就好了），也可以带来一堆奇奇怪怪的问题（比如单词词频差异很大，当前 batch 没有的单词的词向量也被更新；再比如Adam和L2正则结合产生的复杂效果）。用的时候要胆大心细，万一遇到问题找各种魔改 Adam（比如 MaskedAdam[14], AdamW 啥的）抢救。但看一些博客说**adam的相比SGD，收敛快，但泛化能力差，更优结果似乎需要精调SGD**。

`adam,adadelta等, 在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。
adam收敛虽快但是得到的解往往没有sgd+momentum得到的解更好，如果不考虑时间成本的话还是用sgd吧。`

adam是不需要特别调lr，sgd要多花点时间调lr和initial weights。

#### 训练技巧

---

- 要做梯度归一化,即算出来的梯度除以minibatch size
- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15
- dropout对小数据防止过拟合有很好的效果,值一般设为0.5
  - 小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。
- dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置
- 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.
  - sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题
  - 输入0均值，sigmoid函数的输出不是0均值的
- rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整. batch size合适最重要,并不是越大越好
- word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果
- 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: *http://arxiv.org/abs/1505.00387*[17]
- 在数据集很大的情况下，一上来就跑全量数据。建议先用 1/100、1/10 的数据跑一跑，对模型性能和训练时间有个底，外推一下全量数据到底需要跑多久。在没有足够的信心前不做大规模实验。
- GPU 上报错时尽量放在 CPU 上重跑，错误信息更友好
- 在确定初始学习率的时候，从一个很小的值（例如 1e-7）开始，然后每一步指数增大学习率（例如扩大1.05 倍）进行训练。训练几百步应该能观察到损失函数随训练步数呈对勾形，选择损失下降最快那一段的学习率即可。
- 注意实验的可复现性和一致性，注意养成良好的实验记录习惯 ==> 不然如何分析出实验结论。
- 超参上，learning rate 最重要，推荐了解 cosine learning rate 和 cyclic learning rate，其次是 batchsize 和 weight decay。当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。