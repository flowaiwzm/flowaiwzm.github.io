#### SwinFusion 网络

下面从整体架构到各个模块细节，分模块、分步骤地对你给出的这版超轻量化的 SwinFusion 网络再次进行深入分析。内容主要包括：

1. **总体架构概览**
2. **浅特征提取（Shallow Feature Extraction）**
3. **深度特征提取（Deep Feature Extraction）——双支路 Ex_A 和 Ex_B**
4. **跨模态特征融合（Fusion）**
5. **重建/上采样模块（Reconstruction）**
6. **轻量化改动与性能考量**
7. **前向流程（Forward Pass）总结**

------

## 1. 总体架构概览

这份代码实现的 SwinFusion 网络，可以视作一个“双输入、双分支+融合+重建”的 Swin Transformer–风格的图像融合/超分网络。简要流程如下：

1. **输入**：两路灰度图（`A` 和 `B`），通道数均为 1 （`in_chans=1`）。
2. **均值归一化**：针对两路输入各自做减均值、乘缩放。
3. **浅特征提取（Shallow Feature Extraction）**：
   - `A` → `forward_features_Ex_A` → 生成初步特征 $\mathbf{x}$
   - `B` → `forward_features_Ex_B` → 生成初步特征 $\mathbf{y}$
4. **深度特征提取** （双支路 Ex_A、Ex_B）：
   - 各自经过若干层 `RSTB`（Residual Swin Transformer Block），输出形状仍与输入相同（分块后再重排回 $H\times W$）。
   - 对每一路输出做 LayerNorm，然后重构成图像大小特征，用于 Fusion。
5. **跨模态特征融合（Fusion）**：
   - 将 $\mathbf{x}$ 和 $\mathbf{y}$ 拼接后，经若干层 `CRSTB`（Cross Residual Swin Transformer Block）进行互相交叉注意力，将两路信息融合成一路特征 $\mathbf{f}$。
6. **重建/上采样（Reconstruction）**：
   - 接下来把融合后的特征 $\mathbf{f}$ 再送入一组 `RSTB`，形成深度重构特征 $\mathbf{r}$（形状与 $\mathbf{f}$ 相同）。
   - 最后对 $\mathbf{r}$ 执行上采样/纯卷积重建，得到最终的 $(H \times \text{scale}) \times (W \times \text{scale})$ 大小输出。
7. **逆归一化**：上采样结果除以 `img_range` 加回均值，得到最终融合或超分图像。

整体上，这个网络由 **浅提取→深提取（双支路）→融合→深重构→上采样/恢复** 五大阶段串接而成。下面分别拆解。

------

## 2. 浅特征提取（Shallow Feature Extraction）

代码片段（对 `A` 分支，`B` 分支原理相同）：

```python
# in __init__:
self.conv_first1_A = nn.Conv2d(in_chans, embed_dim_temp, 3, 1, 1)
self.conv_first2_A = nn.Conv2d(embed_dim_temp, embed_dim, 3, 1, 1)
self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)

# forward_features_Ex_A:
def forward_features_Ex_A(self, x):
    x = self.lrelu(self.conv_first1_A(x))   # 1 -> embed_dim_temp
    x = self.lrelu(self.conv_first2_A(x))   # embed_dim_temp -> embed_dim
    x_size = (x.shape[2], x.shape[3])
    x = self.patch_embed(x)                 # (B, embed_dim, H, W) -> (B, H*W, embed_dim)
    if self.ape:
        x = x + self.absolute_pos_embed
    x = self.pos_drop(x)
    ...
    x = self.norm_Ex_A(x)
    x = self.patch_unembed(x, x_size)       # (B, H*W, embed_dim) -> (B, embed_dim, H, W)
    return x
```

1. **两层 3×3 卷积 + LeakyReLU**
   - 第一层 `conv_first1_A` 把灰度通道 1 → `embed_dim_temp`（半个 `embed_dim`）。
   - 第二层 `conv_first2_A` 把 `embed_dim_temp` → `embed_dim`。
   - LeakyReLU 激活用来引入非线性。
   - 这两层组合相当于把原始图像 $1\times H \times W$ 提取成 `embed_dim` 维通道数的浅层表示 $(H \times  \text{scale}) \times (W \times \text{scale})$
2. **Patch Embed**
   - `PatchEmbed` 只是把 $(B,{embed dim},,H,,W)$ 按 `patch_size` 划分为多个 patch，然后 `flatten`、`transpose` 得到 $(B,;H\times W,;{embed dim})$ 的序列。
   - 这里 `patch_size=1`，等价于“不做空间降采样”，只是把 `(C, H, W)` 重塑为 `(H*W, C)` 以便送入 Transformer。
3. **可选的绝对位置嵌入（APE） + Dropout**
   - 若 `self.ape=True`，则把全局的绝对位置嵌入 `absolute_pos_embed`（形状 $(1,,H\times W,,{embed_dim})$）加到每个 token 上，再做 `pos_drop`。
4. **通过一系列 `RSTB` 块 + LayerNorm + PatchUnEmbed**
   - 将上面得到的序列 $(B, L=H \times W,C={embed dim})$ 依次进入 `self.layers_Ex_A`（一个 `nn.ModuleList` ，包含若干个 `RSTB` block）。
   - 各个 `RSTB` 块会先将输入做 WindowAttention/Shifted Window Attention，再经 MLP，最后加回残差。
   - 整个循环结束后，用 `self.norm_Ex_A` 做规范化，最后通过 `patch_unembed` 把 $(B,,H\times W,{embed dim})$ 恢复回 $(B,{embed dim},,H,,W)$ 的张量。
   - 因此，浅特征经过深度 Swin Transformer 编码后，再“解码”回原始大小，得到新的特征 $\mathbf{x}_{Ex}\in\mathbb{R}^{{embed_dim}\times H\times W}$。

同理，`forward_features_Ex_B` 针对第二路输入 `B` 也做相同的处理，只不过第二层卷积写成了 `conv_first1_A`、`conv_first2_A`（你实际代码里对 B 分支应该是 `conv_first1_B`、`conv_first2_B`，大致相同）。最终输出 $\mathbf{y}_{Ex}\in\mathbb{R}^{{embed_dim}\times H\times W}$。

------

## 3. 深度特征提取——双支路 Ex_A 和 Ex_B

在 `__init__` 中，针对两路，都构建了相同数目的 `RSTB` 模块序列：

```python
# 构造 Ex_A
self.layers_Ex_A = nn.ModuleList([
    RSTB(dim=embed_dim, input_resolution=(H', W'),
         depth=Ex_depths[i_layer],
         num_heads=Ex_num_heads[i_layer],
         window_size=window_size,
         mlp_ratio=mlp_ratio,
         qkv_bias=qkv_bias, qk_scale=qk_scale,
         drop=drop_rate, attn_drop=attn_drop_rate,
         drop_path=dpr_Ex[...],
         norm_layer=norm_layer,
         downsample=None,
         use_checkpoint=use_checkpoint,
         img_size=img_size,
         patch_size=patch_size,
         resi_connection=resi_connection)
    for i_layer in range(self.Ex_num_layers)
])
self.norm_Ex_A = norm_layer(self.num_features)

# 构造 Ex_B 与 Ex_A 配置完全相同，只是存放在 self.layers_Ex_B
```

1. **`RSTB`（Residual Swin Transformer Block）**

   - 由 `BasicLayer`（一个 Stage，由多个 SwinTransformerBlock + 可选下采样）和短路残差卷积构成。

   - 具体：先将序列输入 `BasicLayer`，产生一个同样形状的序列输出；然后把这个输出转成图像形状 $(B, C, H, W)$，经过一个 1×1 或多层（3conv）的卷积，再加回输入的 $(B, C, H, W)$，构成残差连接。

   - 在此轻量化网络里，你采用了 `resi_connection='1conv'`，所以每个 `RSTB` 最后都会做：

     ```python
     x = self.residual_group(x, x_size) + x_input
     ```

     其中 `self.residual_group` 就是 `BasicLayer(...)` 返回的序列 → patch_unembed 后的 图像特征，再加回输入 $x$（Hot Swap 特征）。

   - `BasicLayer` 内部又循环若干个 `SwinTransformerBlock`。每个 `SwinTransformerBlock` 做如下操作：

     1. **LayerNorm → Shift（或不 Shift） → WindowPartition → WindowAttention → WindowReverse → Unshift**
     2. **残差 + DropPath**
     3. **LayerNorm → MLP → 残差 + DropPath**

2. **双分支独立编码**

   - 两路图像 $A$ 与 $B$ 各自独立地跑完这些 `RSTB` 块，不共享权重（各自 `self.layers_Ex_A` 与 `self.layers_Ex_B`）。
   - 因此，`Ex_A` 支路负责提取 A 图像的深层特征，`Ex_B` 支路负责提取 B 图像的深层特征，彼此不干涉。
   - 在输出阶段，两路都做一次 LayerNorm（`self.norm_Ex_A`/`self.norm_Ex_B`），再做 `patch_unembed`，得回 $(B, {embed dim},,H,,W)$。

**总结**：通过多层 `RSTB` 堆叠，双支路分别把原始浅特征进一步编码为更高抽象能力的特征图，为后续的跨模态融合做准备。

------

## 4. 跨模态特征融合（Fusion）

融合部分集中在 `forward_features_Fusion`，以及对应的 `CRSTB`（Cross Residual Swin Transformer Block）/`Cross_WindowAttention`。代码逻辑如下：

```python
def forward_features_Fusion(self, x, y):
    input_x = x
    input_y = y        
    x_size = (x.shape[2], x.shape[3])  # H, W
    
    # 1) PatchEmbed，将两路特征都映成 (B, H*W, C)
    x = self.patch_embed(x)
    y = self.patch_embed(y)
    if self.ape:
        x = x + self.absolute_pos_embed
        y = y + self.absolute_pos_embed
    x = self.pos_drop(x)
    y = self.pos_drop(y)
    
    # 2) 逐层跨模态 CRSTB
    for layer in self.layers_Fusion:
        x, y = layer(x, y, x_size)
    
    # 3) LayerNorm + PatchUnEmbed
    x = self.norm_Fusion_A(x)
    x = self.patch_unembed(x, x_size)  # (B, C, H, W)
    y = self.norm_Fusion_B(y)
    y = self.patch_unembed(y, x_size)
    
    # 4) 使用 Softmax 加权（注释掉）或者直接拼接
    # x = torch.cat([x, y], 1) → (B, 2*C, H, W)
    x = torch.cat([x, y], dim=1)
    x = self.lrelu(self.conv_after_body_Fusion(x))  # (B, C, H, W)
    return x
```

### 4.1 Cross_WindowAttention

- `Cross_WindowAttention` 与普通的 `WindowAttention` 最大区别在于：

  - **查询（Q）来自一个模态（比如 $x$），而键值对（K,V）来自另一个模态（$!y$）。**

  - 它同样支持窗口划分与相对位置偏置，但计算时：

    ```python
    q = self.q(x)      # (B*, N, C) → 分头 → (B*, nH, N, C/nH)
    kv = self.kv(y)    # → 同理，得到 k, v
    # attention = Softmax( (q @ k^T) / sqrt(d) + relative_bias )
    out = attention @ v
    ```

  - 最终把跨模态注意力加权求和后，再通过输出线性层 `self.proj` → Dropout → 得到融合后的特征。

### 4.2 CRSTB（Cross Residual Swin Transformer Block）

- 每个 `CRSTB` 内部先做“模态内”一轮 LayerNorm，再做“跨模态注意力”→“MLP+残差”：
  1. `x, y` 都先过各自的 `norm1_A`、`norm1_B`，然后 reshape 为 $(B,H,W,C)$，随后做 Shift（与普通 Swin 一样）。
  2. `window_partition` → 生成一组不重叠窗口。
  3. **跨模态注意力**：`attn_A = attn_A(x_windows, y_windows)` → 在 `x` 窗口中，查询来自 `x_windows`，键值来自 `y_windows`；
      `attn_B = attn_B(y_windows, x_windows)` → 对称操作；二者并行计算。
  4. `window_reverse` → 合并回完整特征图。再做 Unshift（如果 `shift_size>0`）。
  5. 把上一步输出 reshape 为 $(B, H*W, C)$。
  6. **残差连接与 MLP**：先把 `shortcut_A+B` 相加，做 `drop_path`，再过 `norm2_A+B` → MLP → 再加回。
- 如果 `shift_size > 0`，则会对特征图做周期滚动来实现 Shifted-Window 模式；如果等于 0，则退化为普通窗口注意力（W-MSA）。
- `CRSTB` 最后没有下采样，下一级依然保持分辨率 $(H,W)$ 不变。

### 4.3 多层 Fusion → 拼通道 → 卷积降维

- `self.layers_Fusion` 是若干个 `CRSTB` 串联，在每一层都同时更新 `x` 和 `y`，二者交叉交换信息。
- 循环结束后，`x`、`y` 都是 `(B, H*W, embed_dim)` 的序列，用 `self.norm_Fusion_A/B` 做归一化，然后 `patch_unembed` 回到 $(B,,{embed dim},,H,,W)$。
- 原本注释掉的 Softmax 加权融合思路是：把 `x`、`y` 分别乘上各自权重后相加—but 目前实际代码直接做 **拼通道** (`torch.cat([x, y], dim=1)`)，得到 `(B, 2*embed_dim, H, W)`。
- 最后用一个 3×3 卷积 `self.conv_after_body_Fusion(nn.LeakyReLU)` 把 `2*embed_dim` → `embed_dim`，完成通道降维与特征融合。

**融合结果** $\mathbf{f}\in\mathbb{R}^{{embed_dim}\times H\times W}$，包含了两路输入图像内外层特征的交叉信息。

------

## 5. 重建/上采样模块（Reconstruction）

融合后生成的特征 $\mathbf{f};(B,embeddim, H, W)$，接下来进入 `forward_features_Re`（深度重构）+ 最后 “上采样/纯卷积”：

```python
def forward_features_Re(self, x):
    x_size = (x.shape[2], x.shape[3])
    x = self.patch_embed(x)             # (B, H*W, C)
    if self.ape:
        x = x + self.absolute_pos_embed
    x = self.pos_drop(x)
    for layer in self.layers_Re:
        x = layer(x, x_size)            # 依次过若干个 RSTB
    x = self.norm_Re(x)                 # (B, H*W, C)
    x = self.patch_unembed(x, x_size)   # (B, C, H, W)

    # —— 下面根据 upsampler 分类  —— 
    if self.upsampler == 'pixelshuffledirect':
        x = self.upsample(x)            # 直接用 UpsampleOneStep:  embed_dim -> (scale^2 * out_ch) -> PixelShuffle
    else:
        # 其余情况（未选’pixelshuffledirect’时），都在 __init__ 定义了 conv_last1/2/3
        x = self.lrelu(self.conv_last1(x))
        x = self.lrelu(self.conv_last2(x))
        x = self.conv_last3(x)
    return x
```

1. **深度重构 (`layers_Re`)**

   - 这部分与前两步相似，只不过这里是用 `RSTB`（单模态版本）对融合后的特征做进一步的 Self-Attention + MLP，增强重构能力。
   - `self.layers_Re` 也是一个 `nn.ModuleList`，共 `Re_num_layers` 个 `RSTB`。每个 `RSTB` 都内部包含若干 `SwinTransformerBlock + 残差卷积`。
   - 流程：`(B, C, H, W)` → `patch_embed` → `LayerNorm` → 循环若干 `RSTB` → `LayerNorm` → `patch_unembed` → 输出 `(B, C, H, W)`。

2. **上采样 / 纯卷积重构**

   - 由于在本例中，`self.upsampler='pixelshuffledirect'`，所以走 `x = self.upsample(x)`，调用的是 `UpsampleOneStep`：

     ```python
     class UpsampleOneStep(nn.Sequential):
         def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):
             m = []
             m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))
             m.append(nn.PixelShuffle(scale))
             super(UpsampleOneStep, self).__init__(*m)
     ```

   - 也就是说，直接做一层 `Conv2d(embed_dim → out_ch * scale^2)`，然后 `PixelShuffle(scale)`，一次性把 $(H, W)$ 放大到 $(H\times\text{scale},,W\times\text{scale})$，输出 `num_out_ch` 通道（在此例中 `out_ch=1`，灰度图）。

   - 如果 `upsampler` 是别的选项（如 `'pixelshuffle'`、`'nearest+conv'`、或默认的 `''`），代码会跳转到 `conv_last1→conv_last2→conv_last3` 三层纯卷积中，生成同样形状 $(B,,{outch},,H,,W)$ 的图像，不做放大。

3. **逆归一化**

   - 最后在 `forward` 中，做 `x = x / self.img_range + self.mean`，将数值映回到原始像素尺度。
   - 输出时做 `x[:, :H*scale, :W*scale]` 裁剪，以确保输出尺寸与原始倍数对齐。

------

## 6. 轻量化改动与性能考量

在 `if __name__ == '__main__'` 中，你实例化了一个 轻量化配置的 SwinFusion：

```python
model = SwinFusion(
    in_chans=1,  
    upscale=2,
    img_size=(64, 64),
    window_size=4,
    img_range=1., 
    depths=[2, 2, 2, 2],        # 每个阶段仅 2 个块
    dim=8,                     # 嵌入维度仅 8（远小于常见的 96 或 128）
    num_heads=[3, 3, 3, 3],    # 注意力头数少
    mlp_ratio=1,               # MLP 扩展倍数 1，相当于隐藏与输出同维
    upsampler='pixelshuffledirect',
    use_checkpoint=True,       # 打开梯度检查点，节省显存
)
```

- **`dim=8`**：嵌入维度非常小，大多数 Swin Transformer 实现里 `dim` 都是 96、128、甚至 192。减到 8，大大降低计算量。
- **`depths=[2,2,2,2]`**：每个阶段只堆 2 个 `SwinTransformerBlock`，原版可能一个阶段要 6、8 个。深度降低，FLOPs 少不少。
- **`window_size=4`**：窗口大小更小，意味着每个窗口只有 $4\times4=16$ 个 token，而不是典型的 $7\times7=49$。
- **`num_heads=[3,3,3,3]`**：每个阶段的头数也非常少，一般设置是 `[3,6,12,24]`。头数减半或更多。
- **`mlp_ratio=1`**：MLP 层的隐藏维就等于输入维，不再做 `4×dim` 或 `2×dim` 的扩展，进一步减少参数。
- **`use_checkpoint=True`**：开启 梯度检查点（checkpointing）可以减少显存占用，但会换成多次计算来换空间，适合显存紧张时。
- **处理输入尺寸 `(64,64)`**：测试时输入很小，反映网络可以更快跑通，容易在本地显存下跑；而生产环境通常是 `(128,128)`、`(256,256)` 等更高分辨率。

**因此，这里的核心轻量化策略就是**：

- **压缩嵌入维度 “dim”**
- **减少每个阶段的块数“depth”**
- **缩小窗口“window_size”**
- **减少每个阶段的注意力头数“num_heads”**
- **把 MLP 隐藏层直接等同于输入，将 `mlp_ratio` 设为 1**
- **统一采用一次性子像素卷积 (`UpsampleOneStep`) 进行上采样，避免多次卷积**

从测试脚本里，你还测量了 GPU 显存占用以及平均推理时间（100 次重复）。结果会给出：

- **参数量（M）**
- **输出尺寸** 正确性验证
- **显存使用情况**
- **平均推理耗时 (ms) → FPS**

------

## 7. 前向流程（Forward Pass）总结

把上述几点串联起来，整个 `forward(A, B)` 的数据流走向可以简要描述成以下步骤：

1. **尺寸与均值处理**

   ```python
   H, W = x.shape[2], x.shape[3]
   x = self.check_image_size(x)    # 保证 H,W 都是 window_size 的整数倍，必要时做边缘 PAD
   y = self.check_image_size(y)
   self.mean_A = self.mean.type_as(x)
   self.mean_B = self.mean.type_as(y)
   self.mean = (mean_A + mean_B) / 2
   x = (x - mean_A) * img_range
   y = (y - mean_B) * img_range
   ```

   - 保证形状可以被窗口均分；并做归一化。

2. **浅特征提取**

   ```python
   x = forward_features_Ex_A(x)  # 输出 x: (B, C, H, W)
   y = forward_features_Ex_B(y)  # 输出 y: (B, C, H, W)
   ```

   - 各自独立跑两层卷积 + 多层 RSTB → 得到 reshape 回 (B,C,H,W) 的特征。

3. **跨模态特征融合**

   ```python
   x = forward_features_Fusion(x, y)  # 输出 x: (B, C, H, W)
   ```

   - 把两路特征做 PatchEmbed → CRSTB 堆叠 → 拼通道 → 3×3 卷积 → 得到融合特征 $\mathbf{f}$。

4. **深度重构**

   ```python
   x = forward_features_Re(x)  # 输出 x: (B, out_ch, H*scale, W*scale) if pixelshuffledirect, else (B, out_ch, H, W)
   ```

   - 先 RSTB 序列细化融合特征 → 再上采样（这里选 ‘pixelshuffledirect’）→ 得到 $H\times2$, $W\times2$ 的结果。

5. **逆归一化 + 裁剪**

   ```python
   x = x / img_range + self.mean
   return x[:, :, :H*scale, :W*scale]
   ```

   - 去掉可能由 `check_image_size` 带来的边缘 PAD，恢复到正好 `(H*scale, W*scale)`。

------

## 附：各模块计算量（FLOPs）大致走向

代码里对若干块都实现了 `.flops()` 方法，方便计算大致的浮点运算量。总体上，模型 FLOPs 包含：

1. **浅特征卷积** $：;2\times (H\times W\times embeddim\times embeddimtemp \times 3^2) $（第一分支卷积 + 第二分支卷积）
2. **`PatchEmbed`/`PatchUnEmbed`**：若开启 LayerNorm，则额外加 $H\times W\times C$
3. **Ex_A/Ex_B 中若干层 `SwinTransformerBlock`**：每个窗口内
   - QKV投影 $(N\times C\times 3C)$
   - 注意力计算 $(nH \times N \times (C/nH)\times N)$
   - 输出映射 $(N \times C \times C)$
   - 以及 MLP 两次 $(N \times C \times C \times mlp_ratio)$。
   - $N = {windowsize}^2$，窗口数 $nW = H\times W / {windowsize}^2$。
4. **Fusion 部分**：类似 Ex，但注意 Q 来自 $x$、KV 来自 $y$，再加拼通道卷积 $(2C \to C)$。
5. **Re 部分**：与 Ex 类似，再叠若干 `RSTB`。
6. **UpsampleOneStep** ：一次卷积 $C\times (upscale^2 \times out_ch)$ + PixelShuffle，PixelShuffle 本身不消耗 FLOPs，只是内存重排。

由于你在轻量化时把 `dim` 设为 8、`window_size` 设为 4、`depths=[2,2,2,2]`，整体 FLOPs 会比常规 SwinIR 小一个数量级。具体运行结果可以借助脚本的 `.flops()` 打印来验证。

------

## 小结

- 本网络结构本质上是一个 **双路输入 → 各自独立编码 → 跨模态融合 → 深度重构 → 上采样** 的 Swin-Transformer–风格模型。
- **浅特征提取** 用“2 层 3×3 卷积 + RSTB 堆叠”完成，负责把原始灰度图编码为高维特征。
- **Fusion** 用 **跨模态窗口注意力** （`Cross_WindowAttention` + `CRSTB`）完成，把两路特征在窗口级别做 Query/KV 交互，最终拼通道加 3×3 卷积降维。
- **深度重构（Re）** 再用一组 `RSTB` 对融合后特征做自注意力细化，最后依据 `upsampler='pixelshuffledirect'` 直接用一次子像素卷积把通道映射+放大，得到 $\times2$ 的输出。
- 整个模型的轻量化策略主要体现在 `dim=8`、`depths=[2,2,2,2]`、`window_size=4`、`num\_heads=[3,3,3,3]`、`mlp\_ratio=1` 这几项设计上，以及只做一次子像素上采样。
- 在测试脚本中，你还测量了**显存占用**（`torch.cuda.memory_allocated()`) 及**推理速度** (100 次平均)，方便评估轻量化效果。

通过以上分模块、分阶段的分析，你应该能对整个 SwinFusion 的具体数据流、各类注意力模块、卷积操作及其轻量化改动有一个非常清晰的认识。希望这个分析能帮助你更好地理解各部分的作用与设计取舍。