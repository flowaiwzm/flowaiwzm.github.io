#### DCGANç½‘ç»œï¼ˆCNN_GANï¼‰

**æ”¹è¿›ç‚¹ï¼š**

- ç”Ÿæˆå™¨*G*å’Œåˆ¤åˆ«å™¨*D*èˆå¼ƒä¼ ç»ŸCNNçš„æ± åŒ–å±‚ï¼Œåˆ¤åˆ«å™¨*D*ä¿ç•™CNNçš„æ•´ä½“æ¡†æ¶è€Œç”Ÿæˆå™¨*G*å°†å·ç§¯å±‚æ›¿æ¢æˆåå·ç§¯å±‚ï¼ˆ*ConvTranspose2d*ï¼‰----ç›®çš„ **Dèˆå¼ƒæ± åŒ–å±‚çš„ç›®çš„ä¿ç•™ç©ºé—´ä¿¡æ¯é¿å…ä¸¢å¤±å±€éƒ¨ä¿¡æ¯ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§Gä½¿ç”¨åå·ç§¯å±‚çš„ç›®çš„å®ç°å¯å­¦ä¹ çš„ä¸Šé‡‡æ ·ã€é¿å…æ£‹ç›˜ä¼ªå½±**
- åœ¨åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨éƒ½é‡‡ç”¨ç”¨**æ‰¹å½’ä¸€åŒ–BNå±‚**æ¥å¤„ç†åˆå§‹åŒ–ä¸è‰¯å¯¼è‡´çš„è®­ç»ƒé—®é¢˜ï¼ŒåŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼›
- åœ¨ç”Ÿæˆå™¨*G*ä¸­é™¤äº†è¾“å‡ºå±‚ä½¿ç”¨**Tanh()**æ¿€æ´»å‡½æ•°ï¼Œå…¶ä½™å±‚å…¨éƒ¨ä½¿ç”¨**ReLu**æ¿€æ´»å‡½æ•°ã€‚è€Œåœ¨åˆ¤åˆ«å™¨*D*ä¸­ï¼Œå‡ºè¾“å‡ºå±‚å¤–æ‰€æœ‰å±‚éƒ½ä½¿ç”¨**LeakyReLu**æ¿€æ´»å‡½æ•°**é˜²æ­¢æ¢¯åº¦ç¨€ç–ã€‚**

![img](https://i-blog.csdnimg.cn/blog_migrate/3b219040b8ee1f562e548c0b4e4efa3c.png)

---

**ReLUå‡½æ•°**

ReLUå‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ä¸ªæ–œå¡å‡½æ•°å…¶å®šä¹‰ä¸ºï¼š

$$ ReLU(x) = \begin{cases} x, & \text{x >0} \\[4ex] 0, & \text{x<0} \end{cases} $$

$ReLU(x)=max(0,x)$

**åŸºæœ¬æ¦‚å¿µ**

- å¯¹äºä»»æ„è¾“å…¥ xï¼Œå¦‚æœ x å°äº0ï¼ŒReLU çš„è¾“å‡ºä¸º0ï¼›å¦‚æœ x å¤§äºæˆ–ç­‰äº0ï¼Œè¾“å‡ºå°±æ˜¯ x æœ¬èº«ã€‚
  è¿™ç§ç®€å•çš„â€œæˆªæ–­â€æ“ä½œä½¿å¾—ReLUå‡½æ•°å…·æœ‰éçº¿æ€§ç‰¹æ€§ï¼ŒåŒæ—¶è®¡ç®—éå¸¸é«˜æ•ˆã€‚
  å¼•å…¥éçº¿æ€§

- åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœåªä½¿ç”¨çº¿æ€§å˜æ¢ï¼ˆä¾‹å¦‚åŠ æƒæ±‚å’Œï¼‰ï¼Œå¤šä¸ªçº¿æ€§å±‚çš„ç»„åˆä¾ç„¶æ˜¯çº¿æ€§å˜æ¢ï¼Œæ— æ³•æ•æ‰æ•°æ®ä¸­çš„å¤æ‚éçº¿æ€§å…³ç³»ã€‚
  ReLUå‡½æ•°çš„éçº¿æ€§æ€§è´¨ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å’Œè¡¨ç¤ºå¤æ‚çš„éçº¿æ€§æ˜ å°„ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
  ç¨€ç–æ¿€æ´»

- å½“è¾“å…¥ä¸ºè´Ÿæ—¶ï¼ŒReLUè¾“å‡º0ï¼Œè¿™å°±å¯¼è‡´åœ¨æŸäº›å±‚ä¸­åªæœ‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¢«æ¿€æ´»ã€‚
  è¿™ç§ç¨€ç–æ€§ä¸ä»…å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œè¿˜èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
  æ¢¯åº¦ä¼ æ’­

- ReLUå‡½æ•°åœ¨ x>0 æ—¶çš„å¯¼æ•°ä¸º1ï¼Œä½¿å¾—æ¢¯åº¦ä¼ æ’­æ—¶ä¸ä¼šå› æ¿€æ´»å‡½æ•°æœ¬èº«å¯¼è‡´æ¢¯åº¦è¡°å‡ã€‚
  ä½†åœ¨ x<0 çš„åŒºåŸŸï¼Œå…¶å¯¼æ•°ä¸º0ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´â€œæ­»ç¥ç»å…ƒâ€é—®é¢˜ï¼Œå³æŸäº›ç¥ç»å…ƒé•¿æ—¶é—´ä¸æ›´æ–°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåæ¥å‘å±•å‡ºäº†æ”¹è¿›ç‰ˆæœ¬ï¼Œå¦‚ Leaky ReLU å’Œ Parametric ReLUã€‚
  ç›´è§‚ç†è§£

- å¯ä»¥å°†ReLUå‡½æ•°çœ‹ä½œæ˜¯ä¸€æ‰‡â€œå¼€å…³â€ï¼šåªæœ‰å½“è¾“å…¥è¶³å¤Ÿå¤§ï¼ˆå¤§äº0ï¼‰æ—¶ï¼Œâ€œå¼€å…³â€æ‰ä¼šæ‰“å¼€ï¼Œè®©ä¿¡å·é€šè¿‡ï¼›å¦åˆ™ï¼Œâ€œå…³æ‰â€ä¿¡å·ï¼Œä¸è®©å®ƒä¼ é€’ä¸‹å»ã€‚
  è¿™ç§æœºåˆ¶ç®€å•ç›´è§‚ï¼Œä¸”åœ¨å®é™…ç¥ç»ç½‘ç»œä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ·±å±‚ç½‘ç»œä¸­èƒ½å¤ŸåŠ é€Ÿè®­ç»ƒå’Œæ”¹å–„æ¢¯åº¦ä¼ æ’­ã€‚

```txt
     (1)ä¼˜ç‚¹ï¼š
     é‡‡ç”¨ ReLU çš„ç¥ç»å…ƒåªéœ€è¦è¿›è¡ŒåŠ ã€ä¹˜å’Œæ¯”è¾ƒçš„æ“ä½œï¼Œè®¡ç®—ä¸Šæ›´åŠ é«˜æ•ˆã€‚
     ReLU å‡½æ•°ä¹Ÿè¢«è®¤ä¸ºå…·æœ‰ç”Ÿç‰©å­¦åˆç†æ€§(Biological Plausibility)ï¼Œæ¯”å¦‚å•ä¾§æŠ‘ åˆ¶ã€å®½å…´å¥‹è¾¹ç•Œ(å³å…´å¥‹ç¨‹åº¦å¯ä»¥éå¸¸é«˜)ã€‚åœ¨ç”Ÿç‰©ç¥ç»ç½‘ç»œä¸­ï¼ŒåŒæ—¶å¤„äºå…´å¥‹çŠ¶æ€çš„ç¥ç»å…ƒéå¸¸ç¨€ç–ã€‚äººè„‘ä¸­åœ¨åŒä¸€æ—¶åˆ»å¤§æ¦‚åªæœ‰ 1% âˆ¼ 4% çš„ç¥ç»å…ƒå¤„äºæ´»è·ƒçŠ¶æ€ã€‚Sigmoid å‹æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´ä¸€ä¸ªéç¨€ç–çš„ç¥ç»ç½‘ç»œï¼Œè€Œ ReLU å´å…·æœ‰å¾ˆå¥½çš„ç¨€ç–æ€§ï¼Œå¤§çº¦ 50% çš„ç¥ç»å…ƒä¼šå¤„äºæ¿€æ´»çŠ¶æ€ã€‚
      åœ¨ä¼˜åŒ–æ–¹é¢ï¼Œç›¸æ¯”äº Sigmoid å‹å‡½æ•°çš„ä¸¤ç«¯é¥±å’Œï¼ŒReLU å‡½æ•°ä¸ºå·¦é¥±å’Œå‡½æ•°ï¼Œ ä¸”åœ¨ ğ‘¥ > 0 æ—¶å¯¼æ•°ä¸º 1ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ é€Ÿæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦ã€‚
    ï¼ˆ2ï¼‰ç¼ºç‚¹ï¼š
       ReLU å‡½æ•°çš„è¾“å‡ºæ˜¯éé›¶ä¸­å¿ƒåŒ–çš„ï¼Œç»™åä¸€å±‚çš„ç¥ç»ç½‘ç»œå¼•å…¥åç½®åç§»ï¼Œ ä¼šå½±å“æ¢¯åº¦ä¸‹é™çš„æ•ˆç‡ã€‚
       æ­¤å¤–ï¼ŒReLU ç¥ç»å…ƒåœ¨è®­ç»ƒæ—¶æ¯”è¾ƒå®¹æ˜“â€œæ­»äº¡â€ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œå¦‚æœå‚æ•°åœ¨ä¸€æ¬¡ä¸æ°å½“çš„æ›´æ–°åï¼Œç¬¬ä¸€ä¸ªéšè—å±‚ä¸­çš„æŸä¸ª ReLU ç¥ç»å…ƒåœ¨æ‰€æœ‰çš„è®­ç»ƒæ•°æ®ä¸Šéƒ½ä¸èƒ½è¢«æ¿€æ´»ï¼Œé‚£ä¹ˆè¿™ä¸ªç¥ç»å…ƒè‡ªèº«å‚æ•°çš„æ¢¯åº¦æ°¸è¿œéƒ½ä¼šæ˜¯ 0ï¼Œåœ¨ä»¥åçš„è®­ç»ƒè¿‡ç¨‹ä¸­æ°¸è¿œä¸èƒ½è¢«æ¿€æ´».è¿™ç§ç°è±¡ç§°ä¸ºæ­»äº¡ ReLU é—®é¢˜(Dying ReLU Problem)ï¼Œå¹¶ä¸”ä¹Ÿæœ‰å¯èƒ½ä¼šå‘ç”Ÿåœ¨å…¶ä»–éšè—å±‚ã€‚
```
**æœ¬è´¨å°±æ˜¯åªæœ‰å½“ç¥ç»å…ƒæ”¶åˆ°æ­£å‘è¶³å¤Ÿå¼ºçš„æ¿€æ´»æ—¶ï¼Œæ‰ä¼šæœ‰æ­£å‘è¾“å‡ºï¼Œå¦åˆ™ä¸º0**

---

**Leaky ReLUå‡½æ•°**

å¸¦æ³„éœ²çš„ ReLU æ”¹è¿›äº†æ ‡å‡† ReLUï¼Œå¯¹äº x<0 çš„éƒ¨åˆ†ï¼Œä¸å†è¾“å‡º 0ï¼Œè€Œæ˜¯è¾“å‡ºä¸€ä¸ªå¾ˆå°çš„çº¿æ€§å€¼ã€‚å¸¸è§å®šä¹‰ä¸ºï¼š

$$ Leaky-ReLU(x) = \begin{cases} x, & \text{x >0} \\[4ex] ax, & \text{x<0} \end{cases} $$aé€šå¸¸å–å€¼ä¸º0.01å°å€¼

$LeakyReLU(x)=max(0,x)+amin(0,x)$

è¿™æ ·ï¼Œå³ä½¿è¾“å…¥ä¸ºè´Ÿï¼Œç¥ç»å…ƒä¹Ÿä¼šäº§ç”Ÿä¸€ä¸ªå°çš„è´Ÿè¾“å‡ºï¼Œå¹¶ä¸”å…¶æ¢¯åº¦ä¸º $\alpha$ï¼ˆè€Œé0ï¼‰ï¼Œä»è€Œä½¿å¾—è¿™äº›ç¥ç»å…ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»èƒ½æ¥æ”¶æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ï¼Œç¼“è§£â€œ**æ­»ç¥ç»å…ƒâ€é—®é¢˜**ã€‚

**ï¼ˆ1ï¼‰æƒé‡æ— æ³•æ›´æ–°**
åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰çš„æ›´æ–°ä¾èµ–äºæ¢¯åº¦ä¿¡æ¯ã€‚å¦‚æœæ¢¯åº¦ä¸º0ï¼Œé‚£ä¹ˆå¯¹åº”æƒé‡ä¸ä¼šå¾—åˆ°ä»»ä½•è°ƒæ•´ï¼Œç¥ç»å…ƒæ— æ³•æ ¹æ®æ•°æ®è°ƒæ•´å…¶å‚æ•°ã€‚

**ï¼ˆ2ï¼‰â€œæ­»â€ç¥ç»å…ƒé—®é¢˜**

å¦‚æœä¸€ä¸ªç¥ç»å…ƒé•¿æ—¶é—´å¤„äº x<0 çš„åŒºåŸŸï¼Œå®ƒçš„è¾“å‡ºå§‹ç»ˆä¸º0ï¼Œä¸”æ¢¯åº¦ä¸º0ï¼Œè¿™æ ·å³ä½¿åç»­è¾“å…¥å‘ç”Ÿå˜åŒ–ï¼Œä¹Ÿæ— æ³•é€šè¿‡æ¢¯åº¦æ›´æ–°ä½¿è¯¥ç¥ç»å…ƒè¾“å‡ºæ¢å¤åˆ°éé›¶çŠ¶æ€ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ­»ç¥ç»å…ƒâ€ï¼ˆDead ReLUï¼‰é—®é¢˜ï¼Œå¯¼è‡´è¯¥ç¥ç»å…ƒåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¤±å»ä½œç”¨ã€‚

**ï¼ˆ3ï¼‰æ— æ³•æ•æ‰ä¿¡æ¯**

ä¸€ä¸ªâ€œæ­»â€ç¥ç»å…ƒä¸å†å¯¹ä»»ä½•è¾“å…¥äº§ç”Ÿå“åº”ï¼Œä»è€Œé™ä½äº†ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½ã€‚

---

**ELUå‡½æ•°**

ELUï¼ˆExponential Linear Unitï¼ŒæŒ‡æ•°çº¿æ€§å•å…ƒï¼‰æ˜¯ä¸€ç§å¸¸ç”¨äºç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°ï¼Œæ—¨åœ¨åŒæ—¶è·å¾—ReLUçš„è®¡ç®—æ•ˆç‡å’Œç¼“è§£â€œæ­»ç¥ç»å…ƒâ€åŠæ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„ä¼˜åŠ¿

$$ ELU(x) = \begin{cases} x, & \text{if x >0} \\[4ex] \gamma(exp(x)-1), & \text{if x<0} \end{cases} $$

$ELU(x)=max(0,x)+min(0,\gamma(exp(x)-1))$

å…¶ä¸­ ğ›¾ â‰¥ 0 æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå†³å®š ğ‘¥ â‰¤ 0 æ—¶çš„é¥±å’Œæ›²çº¿ï¼Œå¹¶è°ƒæ•´è¾“å‡ºå‡å€¼åœ¨ 0 é™„è¿‘

**1ã€æ­£åŒºé—´è¡Œä¸º**

- å½“$x>0$æ—¶ï¼ŒELUå‡½æ•°äºReLUå‡½æ•°ç±»ä¼¼ï¼Œç›´æ¥è¾“å‡ºx
- è¿™éƒ¨åˆ†ä¿ç•™äº†ReLUçš„ç®€å•å’Œé«˜æ•ˆè®¡ç®—çš„ä¼˜ç‚¹

**2ã€è´ŸåŒºé—´è¡Œä¸º**

- å¯¹äºè¾ƒå¤§çš„è´Ÿè¾“å…¥ï¼ŒæŒ‡æ•°è¿…é€Ÿè¶‹äº0
- è¿™æ„å‘³è¿™å‡½æ•°è¾“å‡ºä¸ä¼šä¸€ç›´ä¿æŒä¸º0ï¼Œè€Œæ˜¯è¶‹äºä¸€ä¸ªè´Ÿé¥±å’Œå€¼ï¼Œä»è€Œ ä½¿å¾—è´ŸåŒºé—´çš„æ¢¯åº¦ä¸ä¸º0ï¼Œç¼“è§£äº†â€œæ­»ç¥ç»å…ƒâ€é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒä¸€å®šçš„éçº¿æ€§

**å¸¦æ¥çš„ä¼˜åŠ¿**

- æ”¹å–„æ¢¯åº¦ä¼ æ’­ï¼šç”±äºåœ¨è´ŸåŒºé—´ä¸å®Œå…¨æˆªæ–­æ¢¯åº¦ï¼ŒELUå¯ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—æ·±å±‚ç½‘ç»œè®­ç»ƒæ›´ä¸ºç¨³å®šã€‚
- åŠ å¿«æ”¶æ•›ï¼šå¹³æ»‘çš„è´ŸåŒºåŸŸå’Œè¾“å‡ºçš„é›¶ä¸­å¿ƒåŒ–ï¼ˆå› ä¸ºELUçš„è¾“å‡ºèŒƒå›´æ˜¯ (âˆ’Î±,âˆ) ï¼‰æœ‰åŠ©äºåŠ é€Ÿç½‘ç»œæ”¶æ•›ã€‚
- æŠ—å™ªå£°æ€§ï¼šé€šè¿‡å…è®¸è´Ÿæ¿€æ´»å€¼ï¼ŒELUä¸ºç½‘ç»œæä¾›äº†æ›´ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

---

**Softplus()æ¿€æ´»å‡½æ•°**

$Softplus(x)=log(1+e^x)$

Softpluså‡½æ•°å…¶å¯¼æ•°åˆšå¥½æ—¶Logisticå‡½æ•°ã€‚Softpluså‡½æ•°è™½ç„¶ä¹Ÿå…·æœ‰**å•ä¾§æŠ‘åˆ¶ã€å®½å…´å¥‹è¾¹ç•Œ**çš„ç‰¹å¾ï¼Œå´æ²¡æœ‰ç¨€ç–æ¿€æ´»æ€§

- *å¯ä»¥æŠŠ Softplus æƒ³è±¡ä¸ºâ€œè½¯åŒ–â€åçš„ ReLUã€‚ReLU ä¼šç¡¬æ€§åœ°å°†æ‰€æœ‰è´Ÿå€¼æˆªæ–­ä¸º0ï¼Œè€Œ Softplus åˆ™ä¼šä»¥å¹³æ»‘çš„æ–¹å¼é€æ¸å°†è´Ÿå€¼å‹ç¼©åˆ°æ¥è¿‘0ï¼ŒåŒæ—¶åœ¨æ­£å€¼åŒºåŸŸè¿‘ä¼¼ä¿æŒçº¿æ€§ã€‚è¿™ç§å¹³æ»‘è¿‡æ¸¡å¯ä»¥ä½¿ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´åŠ ç¨³å®šã€‚*
- *åœ¨æŸäº›åº”ç”¨ä¸­ï¼Œå¦‚æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œå½“éœ€è¦æ›´å¹³æ»‘çš„æ¢¯åº¦ä¼ æ’­æˆ–è€…ä¸ºäº†é¿å…åœ¨æ¿€æ´»å‡½æ•°å¤„äº§ç”Ÿæ¢¯åº¦ä¸è¿ç»­çš„é—®é¢˜æ—¶ï¼Œå¯ä»¥é€‰æ‹© Softplus ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚*

**Softplus å‡½æ•°é€šè¿‡ log(1+e^x) çš„å½¢å¼å®ç°äº†å¯¹ ReLU çš„å¹³æ»‘è¿‘ä¼¼ï¼Œæ—¢èƒ½åœ¨æ­£åŒºé—´è¿‘ä¼¼çº¿æ€§ï¼Œåˆèƒ½åœ¨è´ŸåŒºé—´å¹³æ»‘åœ°è¶‹è¿‘äº0ï¼ŒåŒæ—¶ä¿æŒå¤„å¤„å¯å¾®ã€‚å®ƒçš„æ¢¯åº¦æ­£å¥½æ˜¯ Sigmoid å‡½æ•°ï¼Œè¿™ä¸ºæ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­æä¾›äº†å¹³æ»‘ã€ç¨³å®šçš„æ¢¯åº¦ï¼Œä»è€Œå¸®åŠ©ç¥ç»ç½‘ç»œæ›´é«˜æ•ˆåœ°å­¦ä¹ ã€‚**

![img](https://i-blog.csdnimg.cn/direct/8e1d74e0fe864e968041f2711e832adb.png)

``````python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
# æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(0.5, 0.5)
])
train_ds = torchvision.datasets.MNIST('data', train=True, transform=transform, download=True)
train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)
# ç”Ÿæˆå™¨çš„åˆå§‹åŒ–éƒ¨åˆ†
# PSï¼š1.è¾“å‡ºå±‚è¦ç”¨Tanhæ¿€æ´»å‡½æ•°  2.ä½¿ç”¨batchnormï¼Œè§£å†³åˆå§‹åŒ–å·®çš„é—®é¢˜ï¼Œå¸®åŠ©æ¢¯åº¦ä¼ æ’­åˆ°æ¯ä¸€å±‚ï¼Œé˜²æ­¢ç”Ÿæˆå™¨åŒ…æ‰€æœ‰çš„æ ·æœ¬éƒ½æ”¶æ•›åˆ°åŒä¸€ä¸ªç‚¹
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.linear1 = nn.Linear(100, 256 * 7 * 7)
        self.bn1 = nn.BatchNorm1d(256 * 7 * 7)
        # è¿™é‡Œæ˜¯åå·ç§¯ï¼Œstride=2å³è®©å›¾åƒæ”¾å¤§2å€,padding=2å³å¾€é‡Œç¼©å°ä¸¤æ ¼ã€‚
        self.decon1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,
                                         kernel_size=(3, 3),
                                         stride=1,
                                         padding=1)  # (128, 7, 7)
        self.bn2 = nn.BatchNorm2d(128)
        self.decon2 = nn.ConvTranspose2d(128, 64,
                                         kernel_size=(4, 4),
                                         stride=2,
                                         padding=1)  # (64, 14, 14)
        self.bn3 = nn.BatchNorm2d(64)
        self.decon3 = nn.ConvTranspose2d(64, 1,
                                         kernel_size=(4, 4),
                                         stride=2,
                                         padding=1)  # (1, 28, 28)
 
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.bn1(x)
        x = x.view(-1, 256, 7, 7)
        x = F.relu(self.decon1(x))
        x = self.bn2(x)
        x = F.relu(self.decon2(x))
        x = self.bn3(x)
        x = torch.tanh(self.decon3(x))
        return x
    
# åˆ¤åˆ«å™¨çš„åˆå§‹åŒ–éƒ¨åˆ†
# PSï¼š1.è¾“å…¥å±‚ä¸èƒ½ç”¨BN  2.ç”¨LeakyReLUæ¿€æ´»å‡½æ•°  3.ä¸ºäº†é˜²æ­¢åˆ¤åˆ«å™¨è¿‡å¼ºè€Œä¸€è¾¹å€’ï¼Œç”¨dropouté™ä½å…¶å­¦ä¹ æ•ˆæœ
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2)
        self.bn = nn.BatchNorm2d(128)
        self.fc = nn.Linear(128 * 6 * 6, 1)
 
    def forward(self, x):
        x = F.dropout2d(F.leaky_relu_(self.conv1(x)))  # nn.LeakyReLU() æ›´é€‚åˆä½œä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¼šè¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œè€Œä¸ä¼šä¿®æ”¹åŸå§‹æ•°æ®
        x = F.dropout2d(F.leaky_relu_(self.conv2(x)))
        x = self.bn(x)
        x = x.view(-1, 128 * 6 * 6)
        x = self.fc(x)
        return x
    
# åˆå§‹åŒ–æ¨¡å‹ï¼Œå®šä¹‰ä¼˜åŒ–å™¨ï¼ŒæŸå¤±å‡½æ•°
device = 'cuda' if torch.cuda.is_available() else 'cpu'
gen = Generator().to(device)
dis = Discriminator().to(device)
g_optim = optim.Adam(gen.parameters(), lr=1e-4)
d_optim = optim.Adam(dis.parameters(), lr=1e-5)  # PSï¼šå°†åˆ¤åˆ«å™¨çš„å­¦ä¹ ç‡è®¾ç½®å°ä¸€ç‚¹å¯ä»¥å‡å°å…¶å­¦ä¹ é€Ÿåº¦ï¼Œé˜²æ­¢ä¸€è¾¹å€’
loss_fun = torch.nn.MSELoss()
# å®šä¹‰ç»˜å›¾å‡½æ•°
test_input = torch.randn(16, 100, device=device)
 
 
def gen_img_plot(model, test_input):
    prediction = np.squeeze(model(test_input).detach().cpu().numpy())
    plt.figure(figsize=(4, 4))
    for i in range(16):
        plt.subplot(4, 4, i + 1)
        plt.imshow((prediction[i] + 1) / 2, cmap="gray")
        plt.axis("off")
    plt.show()
# è®­ç»ƒGAN
G_loss = []
D_loss = []
for epoch in range(20):
    g_epoch_loss = 0
    d_epoch_loss = 0
    count = len(train_dl)
    for step, (img, _) in enumerate(train_dl):
        img = img.to(device)
        size = img.size(0)
        random_noise = torch.randn(size, 100, device=device)
        # ä¼˜åŒ–åˆ¤åˆ«å™¨
        d_optim.zero_grad()
        # ä¼˜åŒ–çœŸå®å›¾ç‰‡
        real_output = dis(img)
        real_loss = loss_fun(real_output, torch.ones_like(real_output, device=device))
        real_loss.backward()
        # ä¼˜åŒ–ç”Ÿæˆå›¾ç‰‡
        gen_img = gen(random_noise)
        fake_output = dis(gen_img.detach())
        fake_loss = loss_fun(fake_output, torch.zeros_like(fake_output, device=device))
        fake_loss.backward()
 
        d_loss = real_loss + fake_loss
        d_optim.step()
 
        # ä¼˜åŒ–ç”Ÿæˆå™¨
        g_optim.zero_grad()
        fake_output = dis(gen_img)
        g_loss = loss_fun(fake_output, torch.ones_like(fake_output, device=device))
        g_loss.backward()
        g_optim.step()
 
        with torch.no_grad():
            d_epoch_loss += d_loss.item()
            g_epoch_loss += g_loss.item()
 
    with torch.no_grad():
        d_epoch_loss /= count
        g_epoch_loss /= count
        D_loss.append(d_epoch_loss)
        G_loss.append(g_epoch_loss)
        print("Epoch:", epoch)
        gen_img_plot(gen, test_input)
 
plt.plot(D_loss, label="D_loss")
plt.plot(G_loss, label="G_loss")
plt.legend()
plt.show()
``````

