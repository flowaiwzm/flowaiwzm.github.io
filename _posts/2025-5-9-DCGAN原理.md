#### DCGAN网络（CNN_GAN）

**改进点：**

- 生成器*G*和判别器*D*舍弃传统CNN的池化层，判别器*D*保留CNN的整体框架而生成器*G*将卷积层替换成反卷积层（*ConvTranspose2d*）----目的 **D舍弃池化层的目的保留空间信息避免丢失局部信息，提升模型稳定性G使用反卷积层的目的实现可学习的上采样、避免棋盘伪影**
- 在判别器和生成器都采用用**批归一化BN层**来处理初始化不良导致的训练问题，加速模型训练，保证训练稳定性；
- 在生成器*G*中除了输出层使用**Tanh()**激活函数，其余层全部使用**ReLu**激活函数。而在判别器*D*中，出输出层外所有层都使用**LeakyReLu**激活函数**防止梯度稀疏。**

![img](https://i-blog.csdnimg.cn/blog_migrate/3b219040b8ee1f562e548c0b4e4efa3c.png)

---

**ReLU函数**

ReLU函数实际上是一个斜坡函数其定义为：

$$ ReLU(x) = \begin{cases} x, & \text{x >0} \\[4ex] 0, & \text{x<0} \end{cases} $$

$ReLU(x)=max(0,x)$

**基本概念**

- 对于任意输入 x，如果 x 小于0，ReLU 的输出为0；如果 x 大于或等于0，输出就是 x 本身。
  这种简单的“截断”操作使得ReLU函数具有非线性特性，同时计算非常高效。
  引入非线性

- 在神经网络中，如果只使用线性变换（例如加权求和），多个线性层的组合依然是线性变换，无法捕捉数据中的复杂非线性关系。
  ReLU函数的非线性性质使得网络能够学习和表示复杂的非线性映射，从而提高模型的表达能力。
  稀疏激活

- 当输入为负时，ReLU输出0，这就导致在某些层中只有一部分神经元被激活。
  这种稀疏性不仅可以提高计算效率，还能在一定程度上减轻过拟合问题。
  梯度传播

- ReLU函数在 x>0 时的导数为1，使得梯度传播时不会因激活函数本身导致梯度衰减。
  但在 x<0 的区域，其导数为0，这可能会导致“死神经元”问题，即某些神经元长时间不更新。为了解决这个问题，后来发展出了改进版本，如 Leaky ReLU 和 Parametric ReLU。
  直观理解

- 可以将ReLU函数看作是一扇“开关”：只有当输入足够大（大于0）时，“开关”才会打开，让信号通过；否则，“关掉”信号，不让它传递下去。
  这种机制简单直观，且在实际神经网络中表现出色，尤其在深层网络中能够加速训练和改善梯度传播。

```txt
     (1)优点：
     采用 ReLU 的神经元只需要进行加、乘和比较的操作，计算上更加高效。
     ReLU 函数也被认为具有生物学合理性(Biological Plausibility)，比如单侧抑 制、宽兴奋边界(即兴奋程度可以非常高)。在生物神经网络中，同时处于兴奋状态的神经元非常稀疏。人脑中在同一时刻大概只有 1% ∼ 4% 的神经元处于活跃状态。Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 ReLU 却具有很好的稀疏性，大约 50% 的神经元会处于激活状态。
      在优化方面，相比于 Sigmoid 型函数的两端饱和，ReLU 函数为左饱和函数， 且在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。
    （2）缺点：
       ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移， 会影响梯度下降的效率。
       此外，ReLU 神经元在训练时比较容易“死亡”。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活.这种现象称为死亡 ReLU 问题(Dying ReLU Problem)，并且也有可能会发生在其他隐藏层。
```
**本质就是只有当神经元收到正向足够强的激活时，才会有正向输出，否则为0**

---

**Leaky ReLU函数**

带泄露的 ReLU 改进了标准 ReLU，对于 x<0 的部分，不再输出 0，而是输出一个很小的线性值。常见定义为：

$$ Leaky-ReLU(x) = \begin{cases} x, & \text{x >0} \\[4ex] ax, & \text{x<0} \end{cases} $$a通常取值为0.01小值

$LeakyReLU(x)=max(0,x)+amin(0,x)$

这样，即使输入为负，神经元也会产生一个小的负输出，并且其梯度为 $\alpha$（而非0），从而使得这些神经元在训练过程中仍能接收梯度并更新参数，缓解“**死神经元”问题**。

**（1）权重无法更新**
反向传播过程中，参数（权重和偏置）的更新依赖于梯度信息。如果梯度为0，那么对应权重不会得到任何调整，神经元无法根据数据调整其参数。

**（2）“死”神经元问题**

如果一个神经元长时间处于 x<0 的区域，它的输出始终为0，且梯度为0，这样即使后续输入发生变化，也无法通过梯度更新使该神经元输出恢复到非零状态。这种现象被称为“死神经元”（Dead ReLU）问题，导致该神经元在整个训练过程中失去作用。

**（3）无法捕捉信息**

一个“死”神经元不再对任何输入产生响应，从而降低了网络的表达能力和整体性能。

---

**ELU函数**

ELU（Exponential Linear Unit，指数线性单元）是一种常用于神经网络中的激活函数，旨在同时获得ReLU的计算效率和缓解“死神经元”及梯度消失问题的优势

$$ ELU(x) = \begin{cases} x, & \text{if x >0} \\[4ex] \gamma(exp(x)-1), & \text{if x<0} \end{cases} $$

$ELU(x)=max(0,x)+min(0,\gamma(exp(x)-1))$

其中 𝛾 ≥ 0 是一个超参数，决定 𝑥 ≤ 0 时的饱和曲线，并调整输出均值在 0 附近

**1、正区间行为**

- 当$x>0$时，ELU函数于ReLU函数类似，直接输出x
- 这部分保留了ReLU的简单和高效计算的优点

**2、负区间行为**

- 对于较大的负输入，指数迅速趋于0
- 这意味这函数输出不会一直保持为0，而是趋于一个负饱和值，从而 使得负区间的梯度不为0，缓解了“死神经元”问题，同时保持一定的非线性

**带来的优势**

- 改善梯度传播：由于在负区间不完全截断梯度，ELU可以在某种程度上缓解梯度消失问题，使得深层网络训练更为稳定。
- 加快收敛：平滑的负区域和输出的零中心化（因为ELU的输出范围是 (−α,∞) ）有助于加速网络收敛。
- 抗噪声性：通过允许负激活值，ELU为网络提供了更丰富的表达能力和鲁棒性。

---

**Softplus()激活函数**

$Softplus(x)=log(1+e^x)$

Softplus函数其导数刚好时Logistic函数。Softplus函数虽然也具有**单侧抑制、宽兴奋边界**的特征，却没有稀疏激活性

- *可以把 Softplus 想象为“软化”后的 ReLU。ReLU 会硬性地将所有负值截断为0，而 Softplus 则会以平滑的方式逐渐将负值压缩到接近0，同时在正值区域近似保持线性。这种平滑过渡可以使网络在训练过程中更加稳定。*
- *在某些应用中，如深度神经网络中，当需要更平滑的梯度传播或者为了避免在激活函数处产生梯度不连续的问题时，可以选择 Softplus 作为激活函数。*

**Softplus 函数通过 log(1+e^x) 的形式实现了对 ReLU 的平滑近似，既能在正区间近似线性，又能在负区间平滑地趋近于0，同时保持处处可微。它的梯度正好是 Sigmoid 函数，这为梯度下降和反向传播提供了平滑、稳定的梯度，从而帮助神经网络更高效地学习。**

![img](https://i-blog.csdnimg.cn/direct/8e1d74e0fe864e968041f2711e832adb.png)

``````python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
# 数据准备
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(0.5, 0.5)
])
train_ds = torchvision.datasets.MNIST('data', train=True, transform=transform, download=True)
train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)
# 生成器的初始化部分
# PS：1.输出层要用Tanh激活函数  2.使用batchnorm，解决初始化差的问题，帮助梯度传播到每一层，防止生成器包所有的样本都收敛到同一个点
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.linear1 = nn.Linear(100, 256 * 7 * 7)
        self.bn1 = nn.BatchNorm1d(256 * 7 * 7)
        # 这里是反卷积，stride=2即让图像放大2倍,padding=2即往里缩小两格。
        self.decon1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,
                                         kernel_size=(3, 3),
                                         stride=1,
                                         padding=1)  # (128, 7, 7)
        self.bn2 = nn.BatchNorm2d(128)
        self.decon2 = nn.ConvTranspose2d(128, 64,
                                         kernel_size=(4, 4),
                                         stride=2,
                                         padding=1)  # (64, 14, 14)
        self.bn3 = nn.BatchNorm2d(64)
        self.decon3 = nn.ConvTranspose2d(64, 1,
                                         kernel_size=(4, 4),
                                         stride=2,
                                         padding=1)  # (1, 28, 28)
 
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.bn1(x)
        x = x.view(-1, 256, 7, 7)
        x = F.relu(self.decon1(x))
        x = self.bn2(x)
        x = F.relu(self.decon2(x))
        x = self.bn3(x)
        x = torch.tanh(self.decon3(x))
        return x
    
# 判别器的初始化部分
# PS：1.输入层不能用BN  2.用LeakyReLU激活函数  3.为了防止判别器过强而一边倒，用dropout降低其学习效果
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2)
        self.bn = nn.BatchNorm2d(128)
        self.fc = nn.Linear(128 * 6 * 6, 1)
 
    def forward(self, x):
        x = F.dropout2d(F.leaky_relu_(self.conv1(x)))  # nn.LeakyReLU() 更适合作为模型的一部分使用，因为它会返回一个新的张量，而不会修改原始数据
        x = F.dropout2d(F.leaky_relu_(self.conv2(x)))
        x = self.bn(x)
        x = x.view(-1, 128 * 6 * 6)
        x = self.fc(x)
        return x
    
# 初始化模型，定义优化器，损失函数
device = 'cuda' if torch.cuda.is_available() else 'cpu'
gen = Generator().to(device)
dis = Discriminator().to(device)
g_optim = optim.Adam(gen.parameters(), lr=1e-4)
d_optim = optim.Adam(dis.parameters(), lr=1e-5)  # PS：将判别器的学习率设置小一点可以减小其学习速度，防止一边倒
loss_fun = torch.nn.MSELoss()
# 定义绘图函数
test_input = torch.randn(16, 100, device=device)
 
 
def gen_img_plot(model, test_input):
    prediction = np.squeeze(model(test_input).detach().cpu().numpy())
    plt.figure(figsize=(4, 4))
    for i in range(16):
        plt.subplot(4, 4, i + 1)
        plt.imshow((prediction[i] + 1) / 2, cmap="gray")
        plt.axis("off")
    plt.show()
# 训练GAN
G_loss = []
D_loss = []
for epoch in range(20):
    g_epoch_loss = 0
    d_epoch_loss = 0
    count = len(train_dl)
    for step, (img, _) in enumerate(train_dl):
        img = img.to(device)
        size = img.size(0)
        random_noise = torch.randn(size, 100, device=device)
        # 优化判别器
        d_optim.zero_grad()
        # 优化真实图片
        real_output = dis(img)
        real_loss = loss_fun(real_output, torch.ones_like(real_output, device=device))
        real_loss.backward()
        # 优化生成图片
        gen_img = gen(random_noise)
        fake_output = dis(gen_img.detach())
        fake_loss = loss_fun(fake_output, torch.zeros_like(fake_output, device=device))
        fake_loss.backward()
 
        d_loss = real_loss + fake_loss
        d_optim.step()
 
        # 优化生成器
        g_optim.zero_grad()
        fake_output = dis(gen_img)
        g_loss = loss_fun(fake_output, torch.ones_like(fake_output, device=device))
        g_loss.backward()
        g_optim.step()
 
        with torch.no_grad():
            d_epoch_loss += d_loss.item()
            g_epoch_loss += g_loss.item()
 
    with torch.no_grad():
        d_epoch_loss /= count
        g_epoch_loss /= count
        D_loss.append(d_epoch_loss)
        G_loss.append(g_epoch_loss)
        print("Epoch:", epoch)
        gen_img_plot(gen, test_input)
 
plt.plot(D_loss, label="D_loss")
plt.plot(G_loss, label="G_loss")
plt.legend()
plt.show()
``````

