#### 常见维度报错

- **RuntimeError: expand(torch.FloatTensor{[1, 77, 512]}, size=[1, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3) **

正在尝试对一个 **3维张量（[1, 77, 512]）** 调用 `.expand(b, -1)`，而 `expand()` 至少要提供 **和原张量一样多的维度**；1 个样本，77 个 token，每个 token 是 512 维；你只提供了两个维度 `[b, -1]`，PyTorch 不知道如何扩展第三个维度（512），所以抛出错误。

很好，这是一个非常基础但重要的问题，特别是在使用 PyTorch 时。

------

## ✅ `expand` 和 `repeat` 的作用

两者都可以**增加张量的维度或形状**，通常用于将小张量“扩展”或“复制”成更大的张量，但它们有本质区别：

------

## 🔄 `expand()`：**只做视图广播，不复制数据**

```python
a = torch.tensor([[1], [2], [3]])  # shape: [3, 1]
b = a.expand(3, 4)  # shape: [3, 4]
```

- 它不会真正复制内存中的数据，只是**创建一个新视图**。
- 要求**被扩展的维度原始必须为1**，否则会报错。
- 非常高效，但有风险：你不能写入 `b`，否则会触发不可预知的错误。

🔍 举例：

```python
a = torch.tensor([[1], [2], [3]])
b = a.expand(3, 4)
print(b)
# 输出：
# tensor([[1, 1, 1, 1],
#         [2, 2, 2, 2],
#         [3, 3, 3, 3]])
```

------

## 📋 `repeat()`：**实际复制数据，创建新张量**

```python
a = torch.tensor([[1], [2], [3]])  # shape: [3, 1]
b = a.repeat(1, 4)  # shape: [3, 4]
```

- 它会真正复制每一个元素，**生成一个新的张量**。
- 不需要原维度为1，可以任意重复。
- 相对消耗内存更大，但**安全且稳定**。

🔍 举例：

```python
a = torch.tensor([[1], [2], [3]])
b = a.repeat(1, 4)
print(b)
# 输出：
# tensor([[1, 1, 1, 1],
#         [2, 2, 2, 2],
#         [3, 3, 3, 3]])
```

------

## 🧠 总结对比表：

| 特性            | `expand()`                    | `repeat()`                     |
| --------------- | ----------------------------- | ------------------------------ |
| 是否复制数据    | ❌ 不复制（共享数据）          | ✅ 复制（新数据）               |
| 是否占内存      | ✅ 省内存                      | ❌ 占内存多                     |
| 是否要求维度为1 | ✅ 是（只能扩张维度为1的位置） | ❌ 否（任意重复）               |
| 是否可写        | ❌ 不安全写入                  | ✅ 安全                         |
| 效率            | ✅ 更高效（如果只读）          | ⚠️ 慢一些（复制开销）           |
| 使用建议        | ✔️ 如果只读、节省内存          | ✔️ 若需写操作或不满足expand条件 |

------

## ✅ 实战建议：

- **只读/不修改数据？** → 用 `expand()`，省内存。
- **需要写入/修改/训练？** → 用 `repeat()`，更安全。
- **不能确定原始维度是1？** → 用 `repeat()`，避免报错。

---

**在swim-transformer中划分窗口后又重新换成特征图的原因**

在 Swin Transformer 中，「划分窗口后再还原为特征图」的过程是核心设计之一，目的是在 **降低计算复杂度的同时，引入局部性处理**，最终依旧保持全图信息的建模能力。下面我们详细解释这个过程的 **原因** 和 **优势**：

------

## 🌐 1. 为什么划分窗口？

传统 ViT（如原始 Transformer）直接对整张图像做自注意力，其计算复杂度是：

O(N2)\mathcal{O}(N^2)

其中 N=H×WN = H \times W，图像越大，复杂度越高， **很难用于高分辨率图像**。

> **Swin 的做法是**：将图像划分为多个小窗口（如 7×7），仅在每个窗口内做 self-attention。
>  这将复杂度降低为：

$O((M^2)×H×W/M^2)=O(HW)\mathcal{O}((M^2) \times \frac{H \times W}{M^2}) = \mathcal{O}(HW)$

其中 MM 是窗口大小，整体复杂度线性增长。

------

## 🔁 2. 为什么还要重新还原成特征图？

虽然划窗做 attention 降低了复杂度，但每个窗口是 **局部感知**，窗口之间没有交流，会带来以下问题：

| 问题             | 说明                             |
| ---------------- | -------------------------------- |
| 缺少全局建模能力 | 窗口内的信息不够覆盖整张图       |
| 边界效应明显     | 相邻窗口的内容彼此独立，信息割裂 |

### ✅ 解决方法：还原为整张特征图再做 Shifted Window

- Swin Transformer 每两个层之间，会将特征还原为全图再 **做窗口偏移**（shift window）：
  - 第一个层：窗口划分如常规 7×7
  - 第二个层：将图像窗口偏移（如 3 像素），再划分窗口

这样：

- **相邻窗口之间的信息就可以传递**；
- 实现了 **跨窗口的信息交互**；
- 整个网络可以模拟 **全局自注意力** 的效果，但计算量远小于原始方法。

------

## ✅ 总结：窗口还原的必要性

| 步骤         | 目的                               |
| ------------ | ---------------------------------- |
| 划分窗口     | 降低计算复杂度，局部处理           |
| 还原成特征图 | 做 shift 操作，实现跨窗口信息交互  |
| 多层堆叠     | 模拟全局注意力，保持性能和效率平衡 |

Swin Transformer 的“窗口划分 → attention → 还原特征图 → shift 窗口 → 再 attention”的循环，使得它兼具了：

- 高效性（线性复杂度）
- 多尺度建模能力
- 强大的语义理解能力

---

| 阶段 | 注意力类型 | 窗口划分方式 | 是否移位           | 实现效果                           |
| ---- | ---------- | ------------ | ------------------ | ---------------------------------- |
| 1    | W-MSA      | 正常窗口     | 不移位             | 局部注意力，窗口内自注意力         |
| 2    | SW-MSA     | 移位窗口     | 移位（如 shift=3） | 注意力范围重叠，实现跨窗口信息交流 |

相当于对输入图像先进行扁平化处理在进行窗口划分，对每个窗口进行进行注意力计算；利用正常窗口和移位窗口循环计算最后进行还原特征图；再通过交互模块实现两个不同模态图像进行融合

---

很好，这行代码：

```python
x = x.view(B, H, W, C)
```

的作用是：**把一个非空间格式（比如Transformer中扁平化处理后的 patch 序列）重新还原为带有空间结构的特征图**。

------

## 🧠 背景理解

在 Swin Transformer 或 Transformer 结构中，常见有以下两个维度表示方式：

### 1. Transformer 中的序列格式（扁平化）：

```python
x.shape = (B, N, C)  # B：batch，N：token数（例如 H×W），C：通道数
```

通常是：

```python
N = H × W
```

表示把特征图打平成了 patch 序列，用于 Transformer 的注意力机制计算。

------

### 2. CNN/空间特征图格式：

```python
x.shape = (B, C, H, W)  # 用于卷积、可视化、还原结构等操作
```

但有时也会出现：

```python
x.shape = (B, H, W, C)  # 用于窗口划分（window partition）、reshape 可视化等场景
```

------

## 🛠 所以，这段代码的作用是：

把如下格式的张量：

```python
x.shape = (B, N=H×W, C)
```

转换为：

```python
x.shape = (B, H, W, C)
```

这便于做如下操作：

- 📦 窗口划分（window partition）
- 📍 位置映射（窗口坐标对齐）
- 🎨 可视化还原成图像结构
- 🔄 和 `torch.nn.Unfold`, `PixelShuffle` 等空间操作配合使用

------

## ✅ 举个例子：

```python
x = torch.randn(2, 64, 96)  # 2个batch，64个tokens，96维通道
H = W = 8  # 64 = 8×8
x = x.view(2, 8, 8, 96)  # 变成 2×8×8×96
```

变换后可以对每个 8×8 位置做窗口划分或卷积处理等空间操作了。

------

## 📌 总结一句话：

> `x.view(B, H, W, C)` 是在 Transformer 中将扁平化的序列格式（B, N, C）还原为图像空间结构（B, H, W, C），用于后续的空间操作，如窗口 attention、卷积或可视化等。

这是基于swim-transfromer融合网络；而我现在想要利用这个融合网络中swim-transfromer来替换g该以下代码中前面编码和融合这一部分；利用swim-transformer中融合后的向量与Text-prompt相结合在进行解码得到最终的融合图像


``````
from torchsummary import summary
import psutil  # 需要安装：pip install psutil

def print_memory_usage():
    # 打印当前显存占用情况
    allocated = torch.cuda.memory_allocated() / 1024**2
    reserved = torch.cuda.memory_reserved() / 1024**2
    print(f"GPU Memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB")
    
    # 打印CPU内存使用情况
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f"RAM Usage: {mem_info.rss / 1024**2:.2f} MB")

if __name__ == '__main__':
    # 轻量化配置
    upscale = 2  # 降低上采样倍数
    window_size = 4  # 更小的窗口
    height, width = 64, 64  # 更小的输入尺寸
    
    # 超轻量模型配置
    model = SwinFusion(
        in_chans=1,  
        upscale=upscale,
        img_size=(height, width),
        window_size=window_size,
        img_range=1., 
        depths=[1, 1, 1, 1],  # 减少每个阶段的块数
        dim=8,  # 更小的基础维度
        num_heads=[1, 1, 1, 1],  # 减少注意力头数
        mlp_ratio=1,  # 更小的MLP扩展比
        upsampler='pixelshuffledirect',
        use_checkpoint=True,
    )
    
    # 打印模型结构
    print("="*50 + " Model Summary " + "="*50)
    if torch.cuda.is_available():
        model.cuda()
        summary(model, [(1, height, width), (1, height, width)], device='cuda')
    else:
        summary(model, [(1, height, width), (1, height, width)], device='cpu')
    
    # 生成测试数据
    A = torch.randn(1, 1, height, width)
    B = torch.randn(1, 1, height, width)
    
    if torch.cuda.is_available():
        A, B = A.cuda(), B.cuda()
        print("\nInitial GPU Memory:")
        print_memory_usage()
    
    print("\n" + "="*50 + " Forward Pass " + "="*50)
    with torch.no_grad():  # 减少显存占用
        output = model(A, B)
    
    if torch.cuda.is_available():
        print("\nAfter Forward Pass:")
        print_memory_usage()
    
    # 验证输出形状
    expected_shape = (1, 1, height*upscale, width*upscale)
    assert output.shape == expected_shape, \
        f"Output shape {output.shape} != expected {expected_shape}"
    
    print("\n" + "="*50 + " Test Passed " + "="*50)
    print(f"Input shape: {A.shape} -> Output shape: {output.shape}")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

    # 性能测试
    if torch.cuda.is_available():
        print("\n" + "="*50 + " Performance Test " + "="*50)
        starter = torch.cuda.Event(enable_timing=True)
        ender = torch.cuda.Event(enable_timing=True)
        repetitions = 100
        timings = []
        
        # Warm-up
        for _ in range(10):
            _ = model(A, B)
        
        # 测量
        with torch.no_grad():
            for rep in range(repetitions):
                starter.record()
                _ = model(A, B)
                ender.record()
                torch.cuda.synchronize()
                timings.append(starter.elapsed_time(ender))
        
        avg_time = sum(timings) / repetitions
        print(f"Average inference time: {avg_time:.2f}ms")
        print(f"FPS: {1000/avg_time:.2f}")
``````

下面对 `SwinFusion` 构造函数里的各个参数逐一说明它们在融合网络中的作用：

- **`img_size`** (`int` 或 `tuple(int,int)`)
   输入图像的空间尺寸，高度和宽度。如果是单个整数，则认为宽高相同。网络内部会据此计算 patch 分辨率、窗口划分等。
- **`patch_size`** (`int`)
   将特征图划分成若干个小块（patch）时，每个小块的边长。Swin Transformer 的自注意力在这些固定大小的窗口内计算。
- **`in_chans`** (`int`)
   输入图像的通道数。例如灰度图 `1`，RGB 图 `3`，也可是多模态拼接后的通道数。
- **`embed_dim`** (`int`)
   Patch embedding 之后的特征维度。所有 Transformer 块的主干通道数都是这个值；越大表示网络容量越高、特征表达越丰富，但也越耗显存。
- **`Ex_depths`** (`list[int]`)
   “Encoder” 阶段每个子阶段（stage）中残差 Swin Transformer Block（RSTB）的数目。比如 `[6,6]` 就代表两个 Encoder 分支（A、B）各自两层，每层内部串 6 个 Swin 块。
- **`Fusion_depths`** (`list[int]`)
   融合阶段（Cross-Modal Fusion）每个子阶段里跨模态残差块（CRSTB）的数量。列表长度等于要做几次融合，每个元素控制该融合层中块的堆叠深度。
- **`Re_depths`** (`list[int]`)
   Reconstruction（重建）阶段每个子阶段里残差 Swin Transformer Block 数目，与 `Ex_depths` 的概念相同，但它运行在融合后特征上，用于细化融合结果。
- **`Ex_num_heads`** (`list[int]`)
   Encoder 阶段每个子阶段的多头自注意力头数列表，长度与 `Ex_depths` 对应。头数越多，注意力表达越细，但计算量也更大。
- **`Fusion_num_heads`** (`list[int]`)
   融合阶段每个子阶段跨模态注意力的头数，同理调节跨 A→B、B→A 信息交换的细粒度。
- **`Re_num_heads`** (`list[int]`)
   Reconstruction 阶段每个子阶段的注意力头数。
- **`window_size`** (`int`)
   Swin Transformer 中局部窗口的边长（窗口注意力的 spatial 范围）。调小窗口可降低计算量、扩大局部性；调大会加大上下文感受野，也增加开销。
- **`mlp_ratio`** (`float`)
   MLP（Feed-Forward）模块中，隐藏层维度与输入维度的比例。比如 `4.0` 表示先把通道升到 `embed_dim*4`，再降回 `embed_dim`。
- **`qkv_bias`** (`bool`)
   是否在 QKV 投影中添加可学习偏置。一般默认 `True`，能略微提升性能。
- **`qk_scale`** (`float` or `None`)
   是否自定义 scale 因子（默认为 `head_dim ** -0.5`）。不常改动。
- **`drop_rate`** (`float`)
   通用的 Dropout 概率，用于 MLP 和输出投影等处，提升训练鲁棒。
- **`attn_drop_rate`** (`float`)
   注意力权重上的 Dropout 概率，用于 Attention Map 正则化。
- **`drop_path_rate`** (`float`)
   随机深度（Stochastic Depth）比例，越大会随机丢弃更多块的跳层连接，起到层级正则化的作用。
- **`norm_layer`** (`nn.Module`)
   用于替换默认 `nn.LayerNorm` 的归一化层类，如果想用别的归一化（比如 RMSNorm）可以传进来。
- **`ape`** (`bool`)
   是否在 patch embedding 后加上绝对位置编码（Absolute Positional Embedding）。开启后对一些定位敏感任务有帮助。
- **`patch_norm`** (`bool`)
   Patch embed 后是否再加一层归一化；在论文中一般默认为 `True`。
- **`use_checkpoint`** (`bool`)
   是否对每个 Swin 块使用 PyTorch checkpointing 技术，以换算更少的显存消耗（会牺牲一点点前向速度）。
- **`upscale`** (`int`)
   最终图像上采样倍数。`1` 代表不放大（去噪、去压缩伪影），`2/3/4/8` 等代表超分倍数。
- **`img_range`** (`float`)
   输入图像归一化的放缩范围。比如输入在 `[0,1]` 或 `[0,255]` 之间，统一乘以 `img_range` 后进入网络。
- **`upsampler`** (`str`)
   指定重建分支如何放大：
  - `''`：不使用 PixelShuffle，而是用最后的 `conv_last1/2/3` 三层做重建；
  - `'pixelshuffle'`：经典两步上采样 + PixelShuffle；
  - `'pixelshuffledirect'`：一次卷积直接输出可 Shuffle 的多通道，再 PixelShuffle；
  - `'nearest+conv'`：先最近邻插值再做卷积精修。
- **`resi_connection`** (`str`)
   在 RSTB/CRSTB 里，残差分支前用几层卷积做特征“融合”：
  - `'1conv'`：单层 `3×3`；
  - `'3conv'`：先降通道再升通道的三层瓶颈结构，参数更少但更灵活。

------

> 总结：这些参数共同决定了网络的**层数、宽度、注意力粒度、局部感受野、正则化强度、上采样方式**，可以根据算力/显存预算和任务需求自由组合调节。