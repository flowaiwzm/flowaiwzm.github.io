---
title: 'Blog Post number 1'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

注意力机制的发展可以大致分为增强特征聚合和通道空间注意力的结合

1、SE中的挤压和激励


挤压（特征聚集）和激励（特征重新校准）

Ftr（特征变换）主要目的就在于隐式的将通道依赖性嵌入到通道当中，这也导致了一个问题，空间相关性与通道依赖性缠绕在一起导致卷积核在同时捕捉到空间和通道特征时没有明确的分开两者特征。因此采取了挤压和激励的后续操作来使网络能够更敏感地检测和强化有用的特征，而抑制不重要的特征。
特征聚集（挤压） 通过压缩空间维度，提取每个通道的全局信息。通常采用全局平均池化（Global Average Pooling）操作，对每个通道的特征图进行平均池化，得到一个标量值。这个标量值可以看作是对该通道特征的一种全局概括。
特征重新校准（激励） 利用全局信息，生成通道权重，对原始特征进行加权，实现通道注意力，目的就在于完全捕获通道依赖性。
通常使用两个全连接层和一个 sigmoid 激活函数来实现：
第一个全连接层: 将聚集的全局信息映射到一个低维空间，进行降维。
第二个全连接层: 将低维特征映射回原始通道维度。
Sigmoid 激活函数: 将输出值压缩到 0 到 1 之间，作为每个通道的权重系数
 为什么前面用ReLU激活，后面为什么要改用Sigmoid呢？
(1)具有更多的非线性：可以更好地拟合通道间复杂的相关性。
(2)极大地减少了参数量和计算量：降维参数 r 用于控制第一个FC层中的神经元个数，在论文中也是经过多次对比实验得出r=16 时，模型得到的效果是最好的。
(3)由于Sigmoid函数图像的特点，它的值域在0—1之间，那么这样很符合概率分布的特点，最后能够获得 在0—1 之间归一化的权重参数，这样的话再通过乘法逐通道加权到先前的特征图上，使得有用的信息的注意力更趋向于1，而没有用的信息则更趋向于0，得到最后带有注意力权重的特征图。






SE模块通过显式建模通道之间的依赖关系，将通道间的相关性从空间相关性中分离出来，并通过两个步骤（挤压和激励）来重新校准通道特征。这样可以提高网络对重要特征的敏感性，使后续层能够更有效地利用这些特征，从而提升整体模型的性能。

缺点：
降维会对通道注意力机制预测带来副作用，并且没有必要捕获所有渠道之间的关系

2、ECA高效通道注意力

ECA就是为了解决降维的副作用和通道之间依赖关系发明的一种通道注意力机制；


上图中的GAP（全局平局池化）获得聚合特征，再通过执行大小为k的快速1D卷积来生成通道权重，其中k由通道数C的映射自适应确定

在没有降维的通道全局平均池化之后，ECA通过考虑每个通道及其k个邻居来捕获局部跨通道交互。ECA可以通过大小为k的快速1D卷积来有效地实现，其中内核大小k表示局部跨通道交互的覆盖，即，有多少邻居参与一个信道的注意力预测。为了避免通过交叉验证手动调整k，ECA开发了一种自适应确定k的方法，其中交互的覆盖率（即，核大小k）与信道尺寸成比例。